---
title: "Mixed Model"
author: "Jin"
date: "2019-06"
output:
  bookdown::html_document2:
    number_sections: true
    seq_numbering: true
    fig_caption: true
    highlight: haddock
    theme: null
    md_extensions: +east_asian_line_breaks
    keep_md: true
    toc: false
    pandoc_args: ["--filter", "pandoc-crossref", "-M", "eqnPrefix="]
  bookdown::word_document2:
    fig_caption: true
    reference_docx: ./style/word-styles-02.docx
    md_extensions: +east_asian_line_breaks
    pandoc_args: ["--filter", "pandoc-crossref"]
  bookdown::pdf_document2:
    keep_tex: yes
    toc: false
    latex_engine: xelatex
    md_extensions: +east_asian_line_breaks
    pandoc_args: ["--listing", "--filter", "pandoc-crossref"]
css: ./style/markdown.css
autoEqnLabels: true
eqnPrefixTemplate: ($$i$$)
linkReferences: true
bibliography: Bibfile.bib
csl: ./style/chinese-gb7714-2005-numeric.csl
link-citations: true
---

```{r setup,echo=F}
knitr::opts_knit$set(root.dir = getwd())
knitr::opts_chunk$set(echo = FALSE, results = 'hide')
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
```

```{r prepare}
rm(list=ls())
options(digits=4)
options(scipen=100)
graphics.off()
Sys.setlocale("LC_ALL", "Chinese")
```

# Lasso (statistics)

在统计学和机器学习中，Lasso（最小绝对收缩和选择器；也叫Lasso或LASSO）是一种回归分析方法，
它执行变量选择和正则化，以提高其产生的统计模型的预测准确性和可解释性。 它最初于1986年在地
球物理学文献中引入，后来由Robert Tibshirani于1996年独立重新发现并推广，他创造了
这一术语，并对观察到的性能提供了进一步的见解。

Lasso最初是为最小二乘模型制定的，这个简单的例子揭示了估计量的大量行为，包括它与岭回归和
最佳子集选择的关系，以及Lasso系数估计和所谓的软阈值之间的联系。它还表明(像标准线性回归)，
如果协变量是共线的，系数估计不需要是唯一的。

虽然最初是为最小二乘定义的，但lasso正则化很容易以一种简单的方式扩展到广泛的统计模型，包
括广义线性模型、广义估计方程、比例风险模型和M -估计器. Lasso执行子集选择的能力依赖
于约束的形式，并有多种解释，包括几何、贝叶斯统计和凸分析。

LASSO与基础追踪去噪密切相关。

# motivation

为了提高回归模型的预测精度和可解释性，引入了Lasso，通过改变模型拟合过程，只选择提供的协
变量的子集用于最终模型，而不是使用所有协变量。它是在地球物理学中独立开发的，基于先前的
工作，即对系数的拟合和惩罚都使用了 $\ell^{1}$ 惩罚，并由统计学家Robert Tibshirani推广完善。

在lasso之前，最广泛使用的选择包含哪些协变量的方法是逐步选择，它只在某些情况下提高预测
精度，例如只有少数协变量与响应变量有很强的关系。然而，在其他情况下，它会使预测误差更大。
同时，岭回归是当时最流行的提高预测精度的技术。岭回归通过缩小较大的回归系数来减少过度拟
合，从而提高了预测误差，但它不进行协变量选择，因此也不能使模型更具解释性。

Lasso能够实现这两个目标，方法是强制回归系数绝对值之和小于一个固定值，从而强制将某些系数
设置为零，从而有效地选择一个不包含这些系数的更简单的模型。这一思想与岭回归相似，在岭回
归中，系数的平方和必须小于一个固定值，虽然在岭回归的情况下，这只是缩小了系数的大小，并
没有使它们中的任何一个为零。

# 基本形式

Lasso最初是在最小二乘的背景下引入的，首先考虑最小二乘的情况是有指导意义的，因为它在一个
简单的设置中说明了Lasso的许多属性。

考虑由N个案例组成的样本，每个案例由p个协变量和单个响应变量组成。 设 $y_i$ 为响应变量，
$x_i:=(x_1,x_2,\cdots,x_p)^T$ 为第 $i$ 个案例的协变量向量。 然后Lasso的目标是解决

$$\min _{\beta_{0}, \beta}\left\{\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-x_{i}^{T} 
\beta\right)^{2}\right\} \text { subject to } \sum_{j=1}^{p}\left|\beta_{j}\right|\leq t$$

这里 $t$ 是一个预先指定的自由参数，它决定正则化的数量。设 $X$ 为协变量矩阵，
$X_{i j}=\left(x_{i}\right)_{j}$ 并且 $x_{i}^{T}$ 是 $X$ 的第 $i$ 行，则表达式可以写得更紧
凑

$$\min _{\beta_{0}, \beta}\left\{\frac{1}{N}\left\|y-\beta_{0} 1_{N}-X 
\beta\right\|_{2}^{2}\right\} \text { subject to }\|\beta\|_{1} \leq t$$

其中 $\|\beta\|_{p}=\left(\sum_{i=1}^{N}\left|\beta_{i}\right|^{p}\right)^{1 / p}$ 是标准的
$\ell^{p}$形式，$1_{N}$ 是一个 $N \times 1$ 的向量。

用 $\overline{x}$ 表示数据点 $x_i$ 的标量均值，$\overline{y}$ 表示响应变量 $y_i$ 的均值，故 
$\beta_0$ 的估计结果可以通过 $\hat{\beta}_{0}=\overline{y}-\overline{x}^{T} \beta$
得到，因此有

$y_{i}-\hat{\beta}_{0}-x_{i}^{T} \beta=y_{i}-\left(\overline{y}-\overline{x}^{T} \beta\right)-x_{i}^{T} \beta=\left(y_{i}-\overline{y}\right)-\left(x_{i}-\overline{x}\right)^{T} \beta$

因此，使用已居中(使零均值)的变量是标准的。此外，协变量通常是标准化的$\left(\sum_{i=1}^{N} x_{i}^{2}=1\right)$ ，因此解决方案不依赖于测量尺度。
 
把

$$\min _{\beta \in \mathbb{R}^{p}}\left\{\frac{1}{N}\|{y}-{X} {\beta}\|_{2}^{2}\right\} \text { subject to }\|\beta\|_{1} \leq t$$
写成拉格朗日形式

$$\min _{\beta \in \mathbb{R}^{p}}\left\{\frac{1}{N}\|y-X 
\beta\|_{2}^{2}+\lambda\|\beta\|_{1}\right\}$$

是有帮助的。其中 $t$ 和 $\lambda$ 之间的确切关系依赖于数据。

# 正交协变量

下面考虑Lasso估计量的一些基本属性。

首先假设协变量是正交的，以至于$\left(x_{i} | x_{j}\right)=\delta_{i j}$，其中
$(\cdot|\cdot)$ 是内积，并且$\delta_{ij}$ 是kronecker delta克罗内克符号，或者，
同样地，$X^TX=I$ ,然后使用次梯度法可以得到

$$\hat{\beta}_{j}=S_{N \lambda}\left(\hat{\beta}_{j}^{\mathrm{OLS}}\right)=\hat{\beta}_{j}^{\mathrm{OLS}} \max \left(0,1-\frac{N \lambda}{\left|\hat{\beta}_{j}^{\mathrm{OLS}}\right|}\right)$$
其中 $\hat{\beta}^{\text { OLS }}=\left(X^{T} X\right)^{-1} X^{T} y$ .

$S_{\alpha}$ 被称为软阈值运算符，因为它将值转换为零（如果它们足够小则使它们恰
好为零），而不是如硬阈值运算符（通常表示为$H_{\alpha}$）一样，将较小的值设置
为零并且保持较大的值不变。

这可以和岭回归作比较。岭回归中，目标是最小化：

$$\min _{\beta \in \mathbb{R}^{p}}\left\{\frac{1}{N}\|y-X \beta\|_{2}^{2}+\lambda\|\beta\|_{2}^{2}\right\}$$

产生了
$$\hat{\beta}_{j}=(1+N \lambda)^{-1} \hat{\beta}_{j}^{\text { OLS }} $$

因此，岭回归通过均衡性因子 $(1+N \lambda)^{-1}$ 缩减所有的系数并且没有将任何系数
设置为0.它也可以与具有最佳子集选择的回归进行比较，该最佳子集选择的回归的目标是最小化

$$\min _{\beta \in \mathbb{R}^{p}}\left\{\frac{1}{N}\|{y}-{X} {\beta}\|_{2}^{2}+\lambda\|\beta\|_{0}\right\}$$
 
其中 $\|\cdot\|_{0}$ 是 “$\ell^{0}$” 形式, 如果 $z$ 的 $m$ 个分量都是非零的,则 $\ell^{0}$ 
被定义为 $\|z\|=m$ 。在这种情况中，可以表示为

$$\hat{\beta}_{j}=H_{\sqrt{N \lambda}}\left(\hat{\beta}_{j}^{\mathrm{OLS}}\right)=\hat{\beta}_{j}^{\mathrm{OLS}} \mathrm{I}\left(\left|\hat{\beta}_{j}^{\mathrm{OLS}}\right| \geq \sqrt{N \lambda}\right)$$
其中 $H_{\alpha}$ 为硬阈值函数, $I$ 为指标函数(如果参数为真，则为1，否则为0)。

因此，lasso估计值共享岭回归和最佳子集选择回归的估计值具有的特性，因为它们都像岭回归
一样缩小了所有系数的大小，但也将其中一些设置为零，就像在最佳子集选择的情况下一样。此外，虽
然岭回归将所有系数按常数因子进行缩放，但lasso将系数按常数值转换为零，并在达到零时将其设置为零。

# 相关协变量

回到一般的情况中，协变量之间或许不是独立的。考虑一种特殊情况，协变量中有两个变量$j$和$k$,对于
每个观测，都相等，以至于 ${x}_{(j)}={x}_{(k)}$ ，其中${x}_{(j), i}={x}_{i j}$ 。那么，使lasso目标
函数最小的 $\beta_j$ 和 $\beta_k$ 的值并不是唯一确定的。事实上，如果存在某个解决方案
$\hat{\boldsymbol{\beta}}$ ，其中
$\hat{\boldsymbol{\beta}_j} \hat{\boldsymbol{\beta}_k} \geq \mathbf{0}$ ，那么如果 $s\in[0,1]$ 并用 $\hat{\beta}_{j}$  替代
$s\left(\hat{\beta}_{j}+\hat{\beta}_{k}\right)$ ，用 $\hat{\beta}_{k}$ 替代
$(1-s)\left(\hat{\beta}_{j}+\hat{\beta}_{k}\right)$ ，则当保持其他 $\hat{\beta_i}$ 不变时，便
给出解决方案。所以lasso目标函数有一个连续的有效极小值。几种不同的Lasso，包括the Elastic 
Net，已经被设计来解决这个缺点。

# 一般形式

Lasso正则化可以扩展到各种各样的目标函数，如广义线性模型、广义估计方程、比例风险模型和一般的M
估计器。给定目标函数

$$\frac{1}{N} \sum_{i=1}^{N} f\left(x_{i}, y_{i}, \alpha, \beta\right)$$

则lasso正则化版本的估计量将是以下表达式的解决方案

$$\min _{\alpha, \beta} \frac{1}{N} \sum_{i=1}^{N} f\left(x_{i}, y_{i}, \alpha, \beta\right) 
\text { subject to }\|\beta\|_{1} \leq t $$

只有 $\beta$ 被惩罚，而 $\alpha$ 可以自由地取任何允许的值，就像 $\beta_0$ 在基本情况下
没有被惩罚一样。

# 惩罚似然

模型选择也可以通过应用最小角度选择和收缩器(LASSO)惩罚来实现。这是基于从对数似然中减去回归
系数绝对值和的一个倍数($\lambda$)，从而将一些回归系数设置为零(Tibshirani,1996)。一些软件
可(如SAS软件中的PROC GLMSELECT或R包glmnet) (Friedman, Hastie和Tibshirani，2010;SAS
研究所，2016)允许对线性模型、logistic模型和Cox模型进行惩罚拟合，并优化调优参数 $\lambda$ ,
该参数通过交叉验证或信息准则来控制惩罚强度。

Lasso模型被广泛用于高维模型的选择问题,即当工具变量的数量 $k$ 远远超过样本大小 $n$ 时的情
况。在低维问题($k < n$)中，研究人员通常感兴趣的是可解释的回归系数,因此Lasso模型的可解释性
远低于其预测性能。LASSO估计的回归系数受惩罚强度的影响存在一定程度的偏差，但其均方误差(MSE)
可能小于传统估计。由于这种偏差，它们在解释性或描述性模型中的解释是困难的，并且基于放回抽样
过程，如百分位数自助法，的置信区间没有达到它们声称的名义水平。最近Taylor和Tibshirani(2015)
研究了Lasso模型选择后对回归系数进行有效推断的问题，并提出了解决方案。然而，对于该方法的性
能还没有足够的证据，有效的推断需要充分捕捉回归系数抽样分布中的偏差和方差分量。然而，要确定
偏差，需要对偏差分差进行估计，这本质上要求与无偏回归系数进行比较，而无偏回归系数在方差减少
惩罚框架中是无法得到的。因此，Lasso模型的置信区间甚至可以比包含所有工具变量的模型得出的最
大似然区间还要大。这也在Taylor和Tibshirani的例子中得到了证明。

LASSO估计的另一个问题是它依赖于协变量的尺度。因此，LASSO在标准软件中实现时，对单位方差执行
内部标准化，而在它的一些软件实现中，由于回归系数被转换并在原始尺度上报告，所以用户看不到单
位方差。尽管如此，还不清楚这种“一刀切”的标准化是否适合所有建模目的;例如，考虑连续和二元协
变量混合的典型情况。连续协变量可以有不同的偏度，二元协变量可以有显著不同的平衡度。不过这种情况
也得到了解决，例如Porzelius, Schumacher和Binder（2010）。

# gamsel

在许多应用中，假设所有预测器的影响都是由
 $\eta\left(x_{i}\right)=\sum_{j=1}^{p} \beta_{j} x_{i j}$ 形式的简单线性拟合所捕获的，这可
能限制太多。Hastie和Tibshirani(1986)中引入的广义可加模型，通过将广义线性模型的线性预测器
建模为每个变量的更一般函数的和，从而实现了更大的灵活性:

$$\eta\left(x_{i}\right)=\sum_{j=1}^{p} f_{j}\left(x_{i j}\right)$$

其中 $f_j$ 为未知函数，假设其为光滑函数或低复杂度函数。广义可加模型自引入以来被广泛应用，
直到最近因为问题设置受到限制，即预测器的数量 $p$ 与观测数量 $n$ 有一定的相关性。故本文提
出一种广义可加模型的选择和估计方法，与标准方法相比，该方法可以很好地处理更多预测器的问题。

在大型数据设置中，当预测响应变量时，通常可以假定大量测量变量是不相关的或冗余的。因此，最
好生成稀疏的估计值，即对于某些(甚至大多数)预测器 $\hat{f}_{j} \equiv 0$。此外，在许多
应用中，可以合理地假设线性模型 $f_{j}(x)=\beta_{j} x$ 对于许多预测器是足够的。线性关系很容
易解释，线性模型的广泛使用表明线性拟合在实践中通常是很好的近似。对于纯线性模型，lasso 
(Tibshirani, 1996)是执行模型选择的有效正则化形式。包glmnet (Friedman, Hastie和Tibshirani, 
2010)为广义线性模型类的一个重要子类实现了lasso正则化路径。

要求的目标估计量在将每个 $f_j$ 拟合为零，线性或非线性之间进行选择，由研究数据确定。 这样
做，既在适当时保留了线性拟合的可解释性优势，又在存在非线性关系时捕捉了强烈的非线性关系。 
这种方法，我们称之为GAMSEL（广义加法模型选择），是基于优化惩罚（负）对数似然的形式标准，
对于特定的 $\boldsymbol J(f)$ ,有：

$$\hat{f}_{1}, \ldots, \hat{f}_{p}=\underset{f_{1}, \ldots, f_{p} \in \mathcal{F}}{\arg \min }
\ell\left(y ; f_{1}, \ldots, f_{p}\right)+\sum_{j=1}^{p} J\left(f_{j}\right)$$
2.3节给出了惩罚项的具体形式。

# SPLAM

广义部分线性加性模型(GPLAM)是建立预测模型的一种灵活、可解释的方法。它以一种相加的方式组合
特性，允许每个特性对响应变量具有线性或非线性影响。然而，选择哪些特征作为线性特征或非线性特
征通常是已知的。因此，要使GPLAM在对特性知之甚少的情况下成为一种可行的方法，必须克服两个主
要的模型选择挑战:确定模型中包含哪些特性，以及确定哪些特性作非线性处理。引入稀疏部分线性可
加模型(SPLAM)，可将模型拟合和模型选择问题结合成一个凸优化问题。
