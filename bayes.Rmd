---
title: "贝叶斯统计"
author: "Li"
date: "2019-03"
output:
  bookdown::html_document2:
    number_sections: true
    seq_numbering: true
    fig_caption: true
    highlight: haddock
    theme: null
    md_extensions: +east_asian_line_breaks
    keep_md: true
    toc: true
    pandoc_args: ["--filter", "pandoc-crossref", "-M", "eqnPrefix="]
  bookdown::word_document2:
    fig_caption: true
    reference_docx: ./style/word-styles-02.docx
    md_extensions: +east_asian_line_breaks
    pandoc_args: ["--filter", "pandoc-crossref"]
  bookdown::pdf_document2:
    keep_tex: yes
    toc: false
    latex_engine: xelatex
    md_extensions: +east_asian_line_breaks
    pandoc_args: ["--listing", "--filter", "pandoc-crossref"]
css: ./style/markdown.css
autoEqnLabels: true
eqnPrefixTemplate: ($$i$$)
linkReferences: true
bibliography: Bibfile.bib
csl: ./style/chinese-gb7714-2005-numeric.csl
link-citations: true
---

```{r setup,echo=F}
knitr::opts_knit$set(root.dir = getwd())
knitr::opts_chunk$set(echo = FALSE, results = 'hide')
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
```

```{r prepare}
rm(list=ls())
options(digits=4)
options(scipen=100)
graphics.off()
Sys.setlocale("LC_ALL", "Chinese")
```

# 基本概念

## 贝叶斯公式的形式

1. 事件形式： $P(A_i|B)=\frac{P(A_iB)}{P(B)}=\frac{P(B|A_i)P(A_i)}{\sum_{i=1}^KP(B|A_i)P(A_i)}$
其中， $A_i、B$ 表示事件， $A_i$ 互不相容，且 $B \subset \bigcup_{i=1}^KA_i$ 。
2. 随机变量形式：假定随机变量 $\xi、\eta$ 的联合分布密度是 $f(x,y)=f_\xi(x)f_{\eta|\xi}(y|x)$ ，
其中 $f_\xi(x)$ 是 $\xi$ 的边缘密度， $f_{\eta|\xi}(y|x)$ 是当 $\xi=x$ 时 $\eta$ 的条件密度，则：

$$f_{\xi|\eta}(x|y)=\frac{f_{\xi,\eta}(x,y)}{f_{\eta}(y)}=\frac{f_{\eta|\xi}(y|x)f_{\xi}(x)}{\int_{-\infty}^{+\infty}f_{\eta|\xi}(y|x)f_{\xi}(x)dx}$$

$$f_{\eta|\xi}(y|x)=\frac{f_{\xi,\eta}(x,y)}{f_{\xi}(x)}=\frac{f_{\xi|\eta}(x|y)f_{\eta}(y)}{\int_{-\infty}^{+\infty}f_{\xi|\eta}(x|y)f_{\eta}(y)dy}$$

## 贝叶斯方法

1. 将未知参数看成随机变量，记为 $\theta$ ， $\theta$ 已知时，样本 $x_1,\cdots,x_n$ 的联合密度
$p(x_1,\cdots,x_n;\theta)=p(x|\theta)$ 为 $x_1,\cdots,x_n$ 对 $\theta$ 的条件密度；
2. 根据以往对 $\theta$ 的认知，确定先验分布 $\pi(\theta)$ ；
3. 利用 $p(x|\theta)$ 和 $\pi(\theta)$ ，求得 
$h(\theta|x)=\frac{p(x|\theta)\pi(\theta)}{\int_0^1p(x|\theta)\pi(\theta)d\theta}$ ；
4. 利用 $h(\theta|x)$ 对 $\theta$ 做出推断。

注：

1. 无信息先验：对 $\theta$ 没有任何过去的信息可以借鉴，而是希望通过试验结果获得。
无信息指的是没有任何信息可以帮助我们去选用一个特定的分布作为先验分布；
2. 若没有先验知识，则认为 $\theta\sim U[0,1]$ ，这称为贝叶斯假设；
3.  $\theta$ 与 $x_1,\cdots,x_n$ 的联合分布密度的关系：

经典方法中： $p(x_1,\cdots,x_n;\theta)$ 为联合分布密度；

贝叶斯方法： $p(x_1,\cdots,x_n|\theta)=p(x|\theta)$ 为条件分布密度。

# 先验分布的选取

## 基本概念

定义1：若 $z\sim f(x)$ ，记 $f(x)=cg(x)$ ， $c$ 表示常数， $g(x)$ 表示 $f(x)$ 中
与 $x$ 有关的部分，则记 $z\varpropto g(x)$ ，即 $f(x) \varpropto g(x)$ ， $g(x)$ 
就称为分布密度 $f(x)$ 核。只要知道了分布的核，根据 $A\int_Rg(x)dx=1$ 就可以确定
常数A。因此，要求随机变量分布密度，关键是求核。

定义2：当 $x_1\cdots,x_n$ 样本取定后， $p(x_1,\cdots,x_x|\theta)$ 只有 $\theta$ 
在变化，将其看为 $\theta$ 的函数，就称为似然函数，用 $l(\theta|x_1,\cdots,x_n)$ 表示。

根据以上定义可以得到： $h(\theta|x_1,\cdots,x_n)=\frac{\pi(\theta)p(x_1,\cdots,x_n|\theta)}{\int\pi(\theta)p(x_1,\cdots,x_n|\theta)d\theta}$ ，
其中 $x_1,\cdots,x_n$ 是已知常数，分母是与 $\theta$ 无关的常数， $p(x_1,\cdots,x_n|\theta)$ 是 
$l(\theta|x_1,\cdots,x_n)$ ，则 $h(\theta|x_1,\cdots,x_n)\varpropto \pi(\theta)l(\theta|x_1,\cdots,x_n)$ 。

定义3：如果不论 $\theta$ 的先验分布是什么，相应的后验分布 $h(\theta|x_1\cdots,x_n)$ 总是 
$\theta$ 和 $t(x_1,\cdots,x_n)$ 的函数，则对参数 $\theta$ 而言，统计量 $t(x_1,\cdots,x_n)$ 
称为充分统计量。充分统计量可简化数据，降低维数。

定理1（尼曼因子分解定理）：若样本 $x_1,\cdots,x_n$ 对参数 $\theta$ 的条件密度 
$p(x_1,\cdots,x_n|\theta)$ 能表示成 $f(\theta,t(x_1,\cdots,x_n))$ 与 $g(x_1,\cdots,x_n)$ 
的乘积，则 $t(x_1,\cdots,x_n)$ 对参数 $\theta$ 是充分的，反之亦然。

## 先验分布选取方法

### 贝叶斯假设

若没有先验知识，则认为 $\theta\sim U[0,1]$ ，即 $\pi(\theta)=c\theta \in D$ ，
即 $\pi(\theta) \varpropto 1$ 。可以推出： $h(\theta|x_1,\cdots,x_n)\varpropto \pi(\theta)p(x_1,\cdots,x_n|\theta)\varpropto1\cdot l(\theta|x_1,\cdots,x_n)$ 。

以上假设表明，似然函数 $l(\theta|x_1,\cdots,x_n)$ 就是后验密度的核，即 
$h(\theta|x_1,\cdots,x_n) \varpropto l(\theta|x_1,\cdots,x_n)$ 。若参数 $\theta$ 
有充分统计量 $t(x_1,\cdots,x_n)$ ，简记为 $t$ ，则 $h(\theta|t)\varpropto l(\theta|t)$ 。

定义1：满足 $\int_D\pi(\theta)d\theta>0$ 但可以是 $\infty$ 的分布密度为广义分布密度。 

注：贝叶斯假设只有在 $\theta$ 变化范围是无界区域时才会遇到困难，此时需要引入广义分布密度才能处理。

总结：贝叶斯假设为 $h(\theta|x)\varpropto l(\theta|x)$ 

1. 参数 $\theta$ 在有界区域内变化，先验密度 $\pi(\theta)\varpropto 1$ ，称为贝叶斯假设； 
2. 参数 $\theta$ 的变化区域无界，先验密度 $\pi(\theta) \varpropto 1$ ，称为广义贝叶斯假设。

### 共轭分布法

共轭分布法认为，先验分布应取共轭分布才合适。

定义1：设样本 $x_1,\cdots,x_n$ 对参数 $\theta$ 的条件分布为 $p(z_1,\cdots,x_n|\theta)$ ，
如果 $\pi(\theta)$ 决定的后验分布密度 $h(\theta|x_1,\cdots,x_n)$ 与 $\pi(\theta)$ 
的分布密度是同一个类型，则先验分布 $\pi(\theta)$ 称为 $p(x_1,\cdots,x_n|\theta)$ 的共轭分布。

常用共轭先验分布：

|总体分布|参数|共轭先验分布|
|:-------|:---|:-----------|
|二项分布|成功概率|贝塔分布 $\text{Be}(\alpha,\beta)$ |
|泊松分布|均值|伽玛分布 $\text{Ga}(\alpha,\lambda)$ |
|指数分布|均值的倒数|伽玛分布 $\text{Ga}(\alpha,\lambda)$ |
|正态分布（方差已知）|均值|正态分布 $\text{N}(\mu,\sigma^2)$ |
|正态分布（均值已知）|方差|倒伽马分布 $\text{IGa}(\alpha,\lambda)$ |

定义2：先验分布中所含的未知参数称为超参数。一般共轭先验分布中常含有超参数，无信息先验中不含超参数。

确定超参数的方法：

1. 利用先验距
2. 利用先验分位数
3. 利用先验距和先验分位数
4. 其他方法

注：在以上方法中，均是用样本值估计的距和分位数等于共轭先验分布的距和分位数来确定参数。

### 杰弗莱原则

用信息阵行列式的平方根 $\mid{I(\theta)\mid^{1/2}}$ 作为先验分布的核，即 
$\pi(\theta)\varpropto \mid{I(\theta)\mid^{1/2}}$ 。其中 $I(\theta)=E(\frac{\partial\ln p(x_1,\cdots,x_n;\theta)}{\partial\theta_1},\cdots,\frac{\partial\ln p(x_1,\cdots,x_n;\theta)}{\partial\theta_K})$ 。

### 最大熵原则

无信息先验分布应取参数 $\theta$ 的变化范围内熵最大的分布。

定义1：对离散随机变量 $x$ ，取 $a_1,\cdots,a_k,\cdots$ 至多可列个值，且 
$p(x=a_i)=p_i,\quad i=1,2,\cdots$ ，则 $-\sum_{i}p_i\ln p_i$ 称为 $x$ 的熵，
记为 $\text{H}(x)$ ，规定 $0\cdot\ln0=0$ ；对连续型随机变量 $x$ ，若 $x\sim f(x)$ ，
且 $-\int p(x)\ln p(x)dx$ 有意义，则称其为 $x$ 的熵，也记为 $\text{H}(x)$ 。

# 估计及检验

## 基本概念

定义1：在参数 $\theta$ 的变化范围Q上，定义一个二元非负实值函数 $\text{L}(\theta,a)$ ，
称为损失函数，表示用 $a$ 去估计 $\theta$ 的损失。 $E(L(\theta,a))$ 表示期望平均损失，
称为 $a$ 相应的风险函数，用 $\text{R}(\theta)$ 表示。

定义2：若 $\hat{\theta}_*(x)$ 在估计时使得风险函数 $\text{R}(\theta)$ 最小，则称 
$\hat{\theta}_*(x)$ 是一致最小风险估计。

定义3：若 $\hat{\theta}_*(x)$ 使 
$\rho(\hat{\theta}(x),\pi(\theta))=\min_{\hat{\theta}(x)}\rho(\hat{\theta}(x),\pi(\theta))$ ，则称 
$\hat{\theta}_*(x)$ 是针对 $\pi(\theta)$ 的贝叶斯解。其中 
$\rho(\hat{\theta}(x),\pi(\theta))=\int_{\theta(x)}R(\theta)\pi(\theta)d\theta$ 。

定理1：对给定的损失函数 $\text{L}(\theta,a)$ 和先验分布 $\pi(\theta)$ ，若存在 $p(x|\theta)$ ，
记 $R(\hat{\theta}(x)|x)=\int L(\theta,\hat{\theta}(x))p(x|\theta)\pi(\theta)d\theta$ ，
称为 $\hat{\theta}(x)$ 对 $x$ 的后验风险。当 $R(\hat{\theta}(x)|x)=\min_{\hat{\theta}(x)}R(\hat{\theta}(x)|x)$ ， 
$\hat{\theta}_*(x)$ 就是 $\pi(\theta)$  相应的贝叶斯解。相当于，如果有一个 $\theta$ 的估计
使对每个样本值 $x$ ，后验风险都达到最小，它就是所要求的贝叶斯解。

## 估计

### 最大后验估计

使后验密度 $p(\theta|x)$ 达到最大的估计 $\hat{\theta}(x)$ 为最大后验估计。

### 条件期望估计

用后验分布的条件期望值（即后验分布密度的期望值）去估计参数。当不指明损失函数时，
贝叶斯估计为条件期望估计。

### 区间估计

定义1：设参数 $\theta$ 的后验分布为 $h(\theta|x)$ ，对给定的样本 $x$ 和概率 $1-\alpha$ ，
若存在 $\hat{\theta}_L=\hat{\theta}_L(x)、\hat{\theta}_U=\hat{\theta}_U(x)$ ，使 
$p(\hat{\theta}_L\leq\theta\leq\hat{\theta}_U|x)\geq1-\alpha$ ，则称区间 $[\hat{\theta}_L,\hat{\theta}_U]$ 
为参数 $\theta$ 的可信水平为 $1-\alpha$ 的贝叶斯可信区间，其解释为“ $\theta$ 落入 $[\hat{\theta}_L,\hat{\theta}_U]$ 
的概率为 $1-\alpha$ ”，这与经典统计中有所区别，在经典统计中，$\theta$ 不是随机变量，
而是一个固定的数，置信区间解释为这个区间能覆盖住 $\theta$ 的概率为 $1-\alpha$ 。满足 
$p(\theta\geq\hat{\theta_L})\geq1-\alpha$ 的 $\hat{\theta}_L$ 称为 $1-\alpha$ 的可信下限；
满足 $p(\theta\leq\hat{\theta}_U|x)\geq1-\alpha$ 的 $\hat{\theta}_U$ 称为 $1-\alpha$ 的可信上限。

定义2：设参数 $\theta$ 的后验密度为 $h(\theta|x)$ ，对给定的概率 $1-\alpha$ ，若在直线上
存在这样一个子集 $C$ ，满足： $p(C|x)=1-\alpha、\forall\theta_1\in C,\theta_2\notin C$ ，
总有 $h(\theta_1|x)\geq h(\theta_2|x)$ ，则称 $C$ 为 $\theta$ 的可信水平为 $1-\alpha$ 的
最大后验密度可信集，简称 $1-\alpha$ HPD可信集，若 $c$ 是一个区间，则 $C$ 称为 $1-\alpha$ 
最大后验密度可信区间。

## 假设检验

### 假设检验基本思想

算得贝叶斯后验分布 $h(\theta|x)$ 后，即可计算两个假设的后验概率 $\alpha_i=p(\Theta_i|x)d\theta,\quad i=0,1$ 
，之后比较 $\alpha_0、\alpha_1$ 的大小，当 $\frac{\alpha_0}{\alpha_1}>1$ 时，接受 $\text{H}_0$ ；
当 $\frac{\alpha_0}{\alpha_1}<1$ 时，拒绝 $\text{H}_0$ ；当 $\frac{\alpha_0}{\alpha_1}\approx 1$ 
时，尚需进一步抽样获得先验信息。其中 $\frac{\alpha_0}{\alpha_1}$ 称为后验机会比。举例说明其含义， 
$\frac{\alpha_0}{\alpha_1}=8.14$ 表明 $\Theta_0$ 为真的可能要比 $\Theta_1$ 为真的可能大8.14倍。

### 贝叶斯因子

定义1：设两个假设 $\Theta_0、\Theta_1$ 的先验概率分别为 $\pi_0、\pi_1$ ，后验概率分别为 
$\alpha_0、\alpha_1$ ，则称 $B^{\pi}(x)=\frac{\alpha_0/\alpha_1}{\pi_0/\pi_1}$ 为贝叶斯因子，
表征 $x$ 支持 $\Theta_0$ 的程度。

### 简单假设具体情况

1. 简单假设对简单假设（$\text{H}_0:\theta=\theta_0,\quad \text{H}_1:\theta=\theta_1$）

$\alpha_0=\frac{\pi_0p(x|\theta_0)}{\pi_0p(x|\theta_0)+\pi_1p(x|\theta_1)}, \quad \alpha_1=\frac{\pi_1p(x|\theta_1)}{\pi_0p(x|\theta_0)+\pi_1p(x|\theta_1)}$ ，
则 $\frac{\alpha_0}{\alpha_1}=\frac{\pi_0p(x|\theta_0)}{\pi_1p(x|\theta_1)}$ 。

2. 复杂假设对复杂假设（$\text{H}_0:\theta>1,\quad \text{H}_1:\theta\leq 1$）

$g_0(\theta)\varpropto \pi(\theta)I_{\theta_0}(\theta),\quad g_1(\theta)\varpropto \pi(\theta)I_{\theta_1}(\theta)$ 
，则先验分布 $\pi(\theta)=\pi_0g_0(\theta)+\pi_1g_1(\theta)$ ，其中 $\pi_0、\pi_1$ 分别是 
$\Theta_0、\Theta_1$ 上的先验概率， $g_0、g_1$ 分别是 $\Theta_0、\Theta_1$ 上的概率密度函数，
则 $\frac{\alpha_0}{\alpha_1}=\frac{\int_{\theta_0}p(x|\theta)\pi_0g_0(\theta)d\theta}{\int_{\theta_1}p(x|\theta)\pi_1g_1(\theta)d\theta}$

3. 简单假设对复杂假设（$\text{H}_0:\theta=\theta_0,\quad \text{H}_1:\theta \neq \theta_0$）

当 $\theta$ 连续时，用上述假设是不合适的，应为： 
$\text{H}_0:\theta\in[\theta_0-\varepsilon,\theta_0+\varepsilon];\text{H}_1:\theta\notin [\theta_0-\varepsilon,\theta_0+\varepsilon]$ ，
其中 $\varepsilon$ 为一个很小的数。对 $\theta=\theta_0$ 给一个正概率 $\pi_0$ ，
对 $\theta\notin \theta_0$ 给一个加权密度 $\pi_1g_1(\theta)$ ，即 $\theta$ 的先验密度为 
$\pi(\theta)=\pi_0\text{I}_{\theta_0}(\theta)+\pi_1g_1(\theta)$ ，其中 $\text{I}_{\theta_0}(\theta)$ 为 
$\theta=\theta_0$ 的示性函数， $\pi_1=1-\pi_0$ ， $g_1(\theta)$ 为 $\theta\notin \theta_0$ 
上的一个正概率密度函数。样本 $x$ 的边缘分布 
$m(x)=\int_{\theta}p(x|\theta)\pi(\theta)d\theta=\pi_0p(x|\theta_0)+\pi_1m_1(x)$ ，其中 
$m_1(x)=\int_{\theta\notin\theta_0}p(x|\theta)\pi(\theta)d\theta$ ，从而简单假设与
复杂假设的后验概率为： $\pi(\Theta_0|x)=\frac{\pi_0p(x|\theta_0)}{m(x)}、\pi(\Theta_1|x)=\frac{\pi_1m_1(x)}{m(x)}$ 
，则 $\frac{\alpha_0}{\alpha_1}=\frac{\pi_0p(x|\theta_0)}{\pi_1m_1(x)}$ 。

# 贝叶斯决策

## 决策问题三要素

1. 状态集 $\Theta=\{\theta\}$ ，每个 $\theta$ 表示自然界（社会）会出现的一种状态，
状态集用实数表示为状态参数，由一些实数组成为状态空间；
2. 行动集 $\{a\}$ ，每个 $a$ 表示个人对自然界（社会）采取的行动；
3. 收益函数 $\text{Q}(\theta,a)$ 。

## 决策准则

### 行动的容许性

定义1：在给定的决策问题中，假如存在这样的 $a_2$ 使 
$\forall\theta\in\Theta,\exists \text{Q}(\theta,a_2)\geq\text{Q}(\theta,a_1)$ ，
且至少有一个 $\theta$ 使上述不等式严格成立，则称 $a_1$ 是非容许的；假如不存在
这样的 $a_2$ ，则称 $a_1$ 是容许的；假如 $a_1、a_2$ 的收益函数在 $\Theta$ 上
处处相等，则称 $a_1、a_2$ 是等价的。

### 决策准则

1. 悲观准则：每一行动选最小收益，在所有最小收益中选最大值；
2. 乐观准则：每一行动选最大收益，在所有最大收益中选最小值；
3. 折中准则：选乐观系数 $\alpha\in(0,1)$ ，对每一行动 $a$ ，
计算 $H(a)=\alpha\cdot maxQ(\theta,a)+(1-\alpha)\cdot minQ(\theta,a)$ ，取行动 $a$ 使 $H(a)$ 最大。

### 先验期望准则

每种状态给一个先验概率，每种行动下的所有状态乘先验概率得先验期望收益，选最大的；
期望相同时，比较方差，选方差最小的。

### 损失函数

$L(\theta,a)=maxQ(\theta,a)-Q(\theta,a)$ ，为最大收益减去当前收益；

$L(\theta,a)=w(\theta,a)-minw(\theta,a)$ ，为当前支付减去最小支付。

悲观准则：每个行动选最大损失 $maxL(\theta,a)$ ，所有最大损失中选最小的；

先验期望准则：先验期望损失达到最小的行动为最优行动，若先验期望损失相同，
则选先验方差最小的。

常用损失函数：

1. 平方损失函数： $L(\theta,a)=(a-\theta)^2$ ；
2. 加权平方损失函数： $L(\theta,a)=\lambda(\theta)(a-\theta)^2$
3. 线性损失函数： $L(\theta,a)=\begin{cases} K_0(\theta-a), a\leq\theta\\K_1(\theta-a),a>\theta \end{cases}$
4. 绝对值损失函数： $L(\theta,a)=|\theta-a|$
5. 加权线性函数： 
$L(\theta,a)=\begin{cases} K_0(\theta)(\theta-a),a\leq\theta \\K_1(\theta)(\theta-a),a>\theta \end{cases}$
6. 0-1损失函数： $L(\theta,a)=\begin{cases} 0,|a-\theta|\leq\varepsilon\\ 1,|a-\theta|>\varepsilon \end{cases}$
7. 多元二次损失函数： $L(\theta,a)=(a-\theta)^{\prime}A(a-\theta)$ ，其中 
$\theta^{\prime}=(\theta_1,\cdots,\theta_p),a^\prime=(a_1,\cdots,a_p)，A_{p\times p}>0$
8. 二行动线性决策问题的损失函数：状态 $\theta$ 连续或离散， 
$a=\begin{cases} a_1 & \text{接受}\\ a_2 &\text{拒绝} \end{cases}$ ，在每个行动下的收益都是 
$\theta$ 的线性函数，即 $Q(\theta,a)=\begin{cases}b_1+m_1\theta,a=a_1  \\b_2+m_2\theta,a=a_2 \end{cases}$

### 效用函数

常见的效用曲线：

1. 直线型： $U(\alpha m_1+(1-\alpha)m_2)=\alpha U(m_1)+(1-\alpha) U(m_2)$ （风险中立型）
2. 上凸型： $U(\alpha m_1+(1-\alpha)m_2)>\alpha U(m_1)+(1-\alpha) U(m_2)$ （风险偏好型）
3. 下凸型： $U(\alpha m_1+(1-\alpha)m_2)<\alpha U(m_1)+(1-\alpha) U(m_2)$ （风险厌恶型）
4. 混合型：上面几种类型的混合

# 贝叶斯推断

## 离散随机变量的贝叶斯推断

$x_i$ 的先验概率密度为 $g(x_i)$ ，给定 $x_i$ 的条件下 $y_i$ 的后验概率密度为 $f(y_i|x_i)$ ，
则 $x_i、y_i$ 的联合概率密度为 $f(x_i,y_i)=g(x_i)f(y_i|x_i)$ 。给定 $Y=y_j$ ， $x_i$ 的后验
概率密度为 $g(x_i|y_j)=\frac{g(x_i)f(y_j|x_i)}{\sum_{i=1}^{n_i}g(x_i)f(y_j|x_i)}$

例：容器中有5个球，可能一部分是红色，剩下的是绿色，我们不知道有多少球是红色， $x_i=i$ 表示
红球的个数，则 $i=0,1,2,3,4,5$ 。因为没有任何关于红球的信息，所以假设 $X$ 的先验分布为 
$g(0)=g(1)=g(2)=g(3)=g(4)=g(5)=\frac{1}{6}$ 。我们随机抽取一个球，随机变量 $Y=1$ 表示
抽到红球，否则 $Y=0$ ，则 $P(Y=1|x_i=i)=\frac{i}{5}、P(Y=0|x_i=i)=1-\frac{i}{5}$ 。
根据公式可算得后验概率。

注：贝叶斯学派中，不考虑新数据集，只考虑当前数据集，若出现新数据集，则用当前数据集
算得的后验概率作为新数据集的先验概率再进行新后验概率的计算，等价于直接进行计算 
$f(y_1,y_2|x)=f(y_1|x)f(y_2|x,y_1)$ 。

在贝叶斯理论中，常数乘以先验概率并不会影响贝叶斯后验理论的结果，常数乘以似然也不会
改变贝叶斯理论的结果。因为后验概率与先验概率乘以似然成比例，常数在计算后验概率时被消掉了。

## 二项分布比例的贝叶斯推断

假定 $Y\sim \text{B}(n,\pi)$ ，则给定参数 $\pi$ ， $y$ 的条件分布为 
$f(y|\pi)=C_{n}^{y}\pi^y(1-\pi)^{n-y},\quad y=1,\cdots,n$ 。假定 $y$ 是固定的，
是我们观测的成功的次数， $\pi$ 是可变参数，则 $f(y|\pi)=C_{n}^{y}\pi^y(1-\pi)^{n-y},\quad 0\leq\pi\leq1$ 
。根据以上知识可知， $g(\pi|y)=\frac{g(\pi)f(y|\pi)}{\int_0^1g(\pi)f(y|\pi)d\pi}$ 。

### 使用均匀先验

令 $g(\pi)=1,\quad0\leq\pi\leq1$ ，则 $g(\pi|y)=C_{n}^{y}\pi^y(1-\pi)^{n-y},\quad 0\leq\pi\leq1$ 。

### 使用贝塔先验

假定贝塔先验为 $g(\pi;a,b)=\frac{\Gamma(a+b)}{\Gamma(a)+\Gamma(b)}\pi^{a-1}(1-\pi)^{b-1},\quad 0\leq\pi\leq1$ 
，因为后验和先验乘以似然成比例，因此 $g(\pi|y)\varpropto \pi^{a+y-1}(1-\pi)^{b+n-y-1}$ ，令 
$a^\prime=a+y、b^\prime=b+n-y$ ，易知，后验密度也服从贝塔分布，为 
$g(\pi|y)=\frac{\Gamma(n+a+b)}{\Gamma(y+a)\Gamma(n-y+b)}\pi^{y+a-1}(1-\pi)^{n-y+b-1}$
，易知二项分布的共轭先验是贝塔分布。

二项分布的Jefferys先验是 $\text{Be}(\frac{1}{2},\frac{1}{2})$ 。

### 先验选择准则

1. 当有模糊的先验知识时，选择共轭先验；
2. 通过匹配位置参数（均值）和尺度参数（方差）获得真正的先验知识，选择
共轭先验（样本距等于分布距求参数确定分布）；
3. 构建一般的连续先验（分段函数，不同区间有不同先验）。

### 总结后验分布

位置的测量：

1. 后验众数：极大化后验分布的数，若后验分布连续，则求导令其为0。不足之处为，
可能位于分布的一端或附近，不能代表整体分布；有很多局部极大值极小值求导都为0；
2. 后验中位数： $\int_0^{median}g(\pi|y)d\pi=0.5$ 的 $\text{median}$ ，是很好的位置测量；
3. 后验均值： $m^\prime=\int_0^1\pi g(\pi|y)d\pi$ ，当分布厚尾时，会受到很大影响。

散度测量：

1. 后验方差： $var(\pi|y)=\int_0^1(\pi-m^\prime)^2g(\pi|y)d\pi$
2. 后验标准差： $var(\pi|y)^{\frac{1}{2}}$
3. 后验分位数： $k=\int_{-\infty}^{\pi_k} g(\pi|y)d\pi$ ， $\pi_k$ 是第 $k$ 百分位数
4. 四分位差： $IQR=Q_3-Q_1$

### 估计比例$\pi$

后验均值作为对 $\pi$ 的估计，从后验均方误差 $PMSE(\hat{\pi})=var(\pi|y)+(m^\prime-\pi)^2$ 
中可以看出后验均值作为对 $\pi$ 的估计是最好的。

### 贝叶斯可信区间

贝叶斯可信区间为 $m^\prime+z_{\frac{\alpha}{2}}\times s^\prime$ ，其中 $m^\prime$ 
是后验均值， $s^\prime$ 是后验标准差。

### 贝叶斯学派和频率学派对于比例推断的比较

贝叶斯学派认为参数是固定的未知常数，不是随机变量；而贝叶斯学派认为参数是随机变量，
任何贝叶斯推断都是由后验分布计算的。后验均值作为贝叶斯估计，贝叶斯估计比频率学派
的估计有更小的均方误差，即贝叶斯估计更接近真实值。

贝叶斯统计的假设检验：

1. 单边假设检验（ $\text{H}_0:\pi\leq\pi_0;\text{H}_1:\pi>\pi_0$）

后验概率 $P(\pi\leq\pi_0|y)=\int_0^{\pi_0}g(\pi|x)d\pi$ ，若后验概率小于显著性
水平 $\alpha$ ，则拒绝原假设。

2. 双边假设检验（$\text{H}_0:\pi=\pi_0;\text{H}_1:\pi\neq \pi_0$）

置信区间和双边假设的关系：若不拒绝 $\text{H}_0$ ，则说明 $\pi\in[\pi_0-\varepsilon,\pi_0+\varepsilon]$ ；
若拒绝 $\text{H}_0$ ，则说明 $\pi\notin[\pi_0-\varepsilon,\pi_0+\varepsilon]$ ，其中 $\varepsilon$ 
是个很小的数。贝叶斯统计中，用后验分布进行计算，若 $\pi_0$ 落入可信区间，则接受 $\text{H}_0$ ，否则，
拒绝 $\text{H}_0$ 。

## 正态均值的贝叶斯推断

### 有离散先验均值的贝叶斯理论

将多个观察值一个一个算得的后验和用观察值的均值算得的后验相同。

### 有连续先验的正态均值的贝叶斯理论

1. 正态均值Jeffreys先验： $g(\mu)=1$ 。

单一观测 $y$ ：

$$f(y|\mu)\varpropto \exp(-\frac{(y-\mu)^2}{2\sigma^2}),\quad y|\mu\sim N(\mu,\sigma^2)$$

$$g(\mu|y)\varpropto\exp(-\frac{(\mu-y)^2}{2\sigma^2}),\quad \mu|y\sim N(y,\sigma^2)$$

正态随机样本 $y_1,\cdots,y_n$ ：

$$f(\bar{y}|\mu)\varpropto \exp(-\frac{(\bar{y}-\mu)^2}{2\sigma^2/n}),\quad \bar{y}|\mu\sim N(\mu,\frac{\sigma^2}{n})$$

$$g(\mu|\bar{y})\varpropto \exp(-\frac{(\mu-\bar{y})^2}{2\sigma^2/n}),\quad \mu|\bar{y}\sim N(\bar{y},\frac{\sigma^2}{n})$$

2. 正态均值正态先验： $g(\mu)\varpropto \exp(-\frac{(\mu-m)^2}{2s^2}),\quad \mu\sim N(m,s^2)$

单一观测 $y$ ：

$$f(y|\mu)\varpropto \exp(-\frac{(y-\mu)^2}{2\sigma^2}),\quad \mu\sim N(m,s^2)$$

因此， $g(\mu)f(y|\mu)\varpropto \exp(-\frac{(y-\mu)^2}{2\sigma^2}-\frac{(\mu-m)^2}{2s^2})$

正态分布族简单的更新规则：

后验正态分布的均值和方差分别为： 
$m^\prime=\frac{\sigma^2m+s^2y}{\sigma^2+s^2},\quad (s^\prime)^2=\frac{\sigma^2s^2}{\sigma^2+s^2}$ ，
其中正态先验分布 $\text{N}(m,s^2)$ ， $y\sim N(\mu,\sigma^2)$ 。

下一个观测值的预测密度 $f(y_{n+1}|y_1,\cdots,y_n)\sim N(m^\prime,(s^\prime)^2$ ，其中 
$m^\prime=m_n、(s^\prime)^2=\sigma^2+s_n^2$ ，这里 $m_n=\bar{y}、s_n^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2$ 
，相当于 $\bar{y}\sim N(m_n,s_n^2)$ 。

### 正态均值和贝叶斯可信区间

方差已知时，可信区间为 $m^\prime+z_{\frac{\alpha}{2}}s^\prime$

方差未知时，用 $\hat{\sigma}^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2$ 
估计 $\sigma^2$ 带入上式进行求解。

非正态先验，则后验也不是正态的。 $\int_{\mu_L}^{\mu_U}g(\mu|y_1,\cdots,y_n)d\mu=1-\alpha$ ，
最好的 $\mu_L、\mu_U$ 的估计满足 $g(\mu_L|y_1,\cdots,y_n)=g(\mu_U|y_1,\cdots,y_n)$ 。

### 贝叶斯学派和频率学派对于均值推断的比较

频率学派置信区间和有均值先验的贝叶斯可信区间一样。

正态均值的假设检验：

1. 单侧检验（ $\text{H}_0:\mu<\mu_0;\text{H}_1:\mu\geq\mu_0$ ）

零假设的后验概率 $P(\mu<\mu_0|y_1,\cdots,y_n)=\int_{-\infty}^{\mu}g(\mu|y_1,\cdots,y_n)d\mu$ ，
当 $g(\mu|y_1,\cdots,y_n)\sim N(m^\prime,(s^\prime)^2)$ 时，上式
$=P(\frac{\mu-m^\prime}{s^\prime}\leq\frac{\mu_0-m^\prime}{s^\prime})=P(z\leq\frac{\mu_0-m^\prime}{s^\prime})$
，后验概率小于显著性水平 $\alpha$ 时，拒绝原假设，否则，不拒绝原假设。

2.双侧检验（ $\text{H}_0:\mu=\mu_0;\text{H}_1:\mu\neq\mu_0$ ）

$\mu_0$ 在可信区间内，接受原假设，否则，拒绝原假设。原假设设定标准为原来就有的，
备择假设为希望去检验的。

## 简单线性回归的贝叶斯推断




# 参考文献
[//]: # (\bibliography{Bibfile})