---
title: "贝叶斯统计"
author: "Li"
date: "2019-03"
output:
  bookdown::html_document2:
    number_sections: true
    seq_numbering: true
    fig_caption: true
    highlight: haddock
    theme: null
    md_extensions: +east_asian_line_breaks
    keep_md: true
    toc: true
    pandoc_args: ["--filter", "pandoc-crossref", "-M", "eqnPrefix="]
  bookdown::word_document2:
    fig_caption: true
    reference_docx: ./style/word-styles-02.docx
    md_extensions: +east_asian_line_breaks
    pandoc_args: ["--filter", "pandoc-crossref"]
  bookdown::pdf_document2:
    keep_tex: yes
    toc: false
    latex_engine: xelatex
    md_extensions: +east_asian_line_breaks
    pandoc_args: ["--listing", "--filter", "pandoc-crossref"]
css: ./style/markdown.css
autoEqnLabels: true
eqnPrefixTemplate: ($$i$$)
linkReferences: true
bibliography: Bibfile.bib
csl: ./style/chinese-gb7714-2005-numeric.csl
link-citations: true
---

```{r setup,echo=F}
knitr::opts_knit$set(root.dir = getwd())
knitr::opts_chunk$set(echo = FALSE, results = 'hide')
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
```

```{r prepare}
rm(list=ls())
options(digits=4)
options(scipen=100)
graphics.off()
Sys.setlocale("LC_ALL", "Chinese")
```

**To Do**

- [x] 逆卡方分布
- [x] 均值方差均未知的贝叶斯后验的推导
- [x] 思想基本可以看懂，但是不会自己写代码，感觉还是分布密度和判断准则不会计算的问题


# 基本概念

## 贝叶斯公式的形式

1. 事件形式： $P(A_i|B)=\frac{P(A_iB)}{P(B)}=\frac{P(B|A_i)P(A_i)}{\sum_{i=1}^KP(B|A_i)P(A_i)}$
其中， $A_i、B$ 表示事件， $A_i$ 互不相容，且 $B \subset \bigcup_{i=1}^KA_i$ 。
2. 随机变量形式：假定随机变量 $\xi、\eta$ 的联合分布密度是 $f(x,y)=f_\xi(x)f_{\eta|\xi}(y|x)$ ，
其中 $f_\xi(x)$ 是 $\xi$ 的边缘密度， $f_{\eta|\xi}(y|x)$ 是当 $\xi=x$ 时 $\eta$ 的条件密度，则：

$$f_{\xi|\eta}(x|y)=\frac{f_{\xi,\eta}(x,y)}{f_{\eta}(y)}=\frac{f_{\eta|\xi}(y|x)f_{\xi}(x)}{\int_{-\infty}^{+\infty}f_{\eta|\xi}(y|x)f_{\xi}(x)dx}$$

$$f_{\eta|\xi}(y|x)=\frac{f_{\xi,\eta}(x,y)}{f_{\xi}(x)}=\frac{f_{\xi|\eta}(x|y)f_{\eta}(y)}{\int_{-\infty}^{+\infty}f_{\xi|\eta}(x|y)f_{\eta}(y)dy}$$

## 贝叶斯方法

1. 将未知参数看成随机变量，记为 $\theta$ ， $\theta$ 已知时，样本 $x_1,\cdots,x_n$ 的联合密度
$p(x_1,\cdots,x_n;\theta)=p(x|\theta)$ 为 $x_1,\cdots,x_n$ 对 $\theta$ 的条件密度；
2. 根据以往对 $\theta$ 的认知，确定先验分布 $\pi(\theta)$ ；
3. 利用 $p(x|\theta)$ 和 $\pi(\theta)$ ，求得 
$h(\theta|x)=\frac{p(x|\theta)\pi(\theta)}{\int_0^1p(x|\theta)\pi(\theta)d\theta}$ ；
4. 利用 $h(\theta|x)$ 对 $\theta$ 做出推断。

注：

1. 无信息先验：对 $\theta$ 没有任何过去的信息可以借鉴，而是希望通过试验结果获得。
无信息指的是没有任何信息可以帮助我们去选用一个特定的分布作为先验分布；
2. 若没有先验知识，则认为 $\theta\sim U[0,1]$ ，这称为贝叶斯假设；
3.  $\theta$ 与 $x_1,\cdots,x_n$ 的联合分布密度的关系：

经典方法中： $p(x_1,\cdots,x_n;\theta)$ 为联合分布密度；

贝叶斯方法： $p(x_1,\cdots,x_n|\theta)=p(x|\theta)$ 为条件分布密度。

# 先验分布的选取

## 基本概念

定义1：若 $z\sim f(x)$ ，记 $f(x)=cg(x)$ ， $c$ 表示常数， $g(x)$ 表示 $f(x)$ 中
与 $x$ 有关的部分，则记 $z\varpropto g(x)$ ，即 $f(x) \varpropto g(x)$ ， $g(x)$ 
就称为分布密度 $f(x)$ 核。只要知道了分布的核，根据 $A\int_Rg(x)dx=1$ 就可以确定
常数A。因此，要求随机变量分布密度，关键是求核。

定义2：当 $x_1\cdots,x_n$ 样本取定后， $p(x_1,\cdots,x_x|\theta)$ 只有 $\theta$ 
在变化，将其看为 $\theta$ 的函数，就称为似然函数，用 $l(\theta|x_1,\cdots,x_n)$ 表示。

根据以上定义可以得到： $h(\theta|x_1,\cdots,x_n)=\frac{\pi(\theta)p(x_1,\cdots,x_n|\theta)}{\int\pi(\theta)p(x_1,\cdots,x_n|\theta)d\theta}$ ，
其中 $x_1,\cdots,x_n$ 是已知常数，分母是与 $\theta$ 无关的常数， $p(x_1,\cdots,x_n|\theta)$ 是 
$l(\theta|x_1,\cdots,x_n)$ ，则 $h(\theta|x_1,\cdots,x_n)\varpropto \pi(\theta)l(\theta|x_1,\cdots,x_n)$ 。

定义3：如果不论 $\theta$ 的先验分布是什么，相应的后验分布 $h(\theta|x_1\cdots,x_n)$ 总是 
$\theta$ 和 $t(x_1,\cdots,x_n)$ 的函数，则对参数 $\theta$ 而言，统计量 $t(x_1,\cdots,x_n)$ 
称为充分统计量。充分统计量可简化数据，降低维数。

定理1（尼曼因子分解定理）：若样本 $x_1,\cdots,x_n$ 对参数 $\theta$ 的条件密度 
$p(x_1,\cdots,x_n|\theta)$ 能表示成 $f(\theta,t(x_1,\cdots,x_n))$ 与 $g(x_1,\cdots,x_n)$ 
的乘积，则 $t(x_1,\cdots,x_n)$ 对参数 $\theta$ 是充分的，反之亦然。

## 先验分布选取方法

### 贝叶斯假设

若没有先验知识，则认为 $\theta\sim U[0,1]$ ，即 $\pi(\theta)=c\theta \in D$ ，
即 $\pi(\theta) \varpropto 1$ 。可以推出： $h(\theta|x_1,\cdots,x_n)\varpropto \pi(\theta)p(x_1,\cdots,x_n|\theta)\varpropto1\cdot l(\theta|x_1,\cdots,x_n)$ 。

以上假设表明，似然函数 $l(\theta|x_1,\cdots,x_n)$ 就是后验密度的核，即 
$h(\theta|x_1,\cdots,x_n) \varpropto l(\theta|x_1,\cdots,x_n)$ 。若参数 $\theta$ 
有充分统计量 $t(x_1,\cdots,x_n)$ ，简记为 $t$ ，则 $h(\theta|t)\varpropto l(\theta|t)$ 。

定义1：满足 $\int_D\pi(\theta)d\theta>0$ 但可以是 $\infty$ 的分布密度为广义分布密度。 

注：贝叶斯假设只有在 $\theta$ 变化范围是无界区域时才会遇到困难，此时需要引入广义分布密度才能处理。

总结：贝叶斯假设为 $h(\theta|x)\varpropto l(\theta|x)$ 

1. 参数 $\theta$ 在有界区域内变化，先验密度 $\pi(\theta)\varpropto 1$ ，称为贝叶斯假设； 
2. 参数 $\theta$ 的变化区域无界，先验密度 $\pi(\theta) \varpropto 1$ ，称为广义贝叶斯假设。

### 共轭分布法

共轭分布法认为，先验分布应取共轭分布才合适。

定义1：设样本 $x_1,\cdots,x_n$ 对参数 $\theta$ 的条件分布为 $p(z_1,\cdots,x_n|\theta)$ ，
如果 $\pi(\theta)$ 决定的后验分布密度 $h(\theta|x_1,\cdots,x_n)$ 与 $\pi(\theta)$ 
的分布密度是同一个类型，则先验分布 $\pi(\theta)$ 称为 $p(x_1,\cdots,x_n|\theta)$ 的共轭分布。

常用共轭先验分布：

|总体分布|参数|共轭先验分布|
|:-------|:---|:-----------|
|二项分布|成功概率|贝塔分布 $\text{Be}(\alpha,\beta)$ |
|泊松分布|均值|伽玛分布 $\text{Ga}(\alpha,\lambda)$ |
|指数分布|均值的倒数|伽玛分布 $\text{Ga}(\alpha,\lambda)$ |
|正态分布（方差已知）|均值|正态分布 $\text{N}(\mu,\sigma^2)$ |
|正态分布（均值已知）|方差|倒伽马分布 $\text{IGa}(\alpha,\lambda)$ |

定义2：先验分布中所含的未知参数称为超参数。一般共轭先验分布中常含有超参数，无信息先验中不含超参数。

确定超参数的方法：

1. 利用先验距
2. 利用先验分位数
3. 利用先验距和先验分位数
4. 其他方法

注：在以上方法中，均是用样本值估计的距和分位数等于共轭先验分布的距和分位数来确定参数。

### 杰弗莱原则

用信息阵行列式的平方根 $\mid{I(\theta)\mid^{1/2}}$ 作为先验分布的核，即 
$\pi(\theta)\varpropto \mid{I(\theta)\mid^{1/2}}$ 。其中 $I(\theta)=E(\frac{\partial\ln p(x_1,\cdots,x_n;\theta)}{\partial\theta_1},\cdots,\frac{\partial\ln p(x_1,\cdots,x_n;\theta)}{\partial\theta_K})$ 。

### 最大熵原则

无信息先验分布应取参数 $\theta$ 的变化范围内熵最大的分布。

定义1：对离散随机变量 $x$ ，取 $a_1,\cdots,a_k,\cdots$ 至多可列个值，且 
$p(x=a_i)=p_i,\quad i=1,2,\cdots$ ，则 $-\sum_{i}p_i\ln p_i$ 称为 $x$ 的熵，
记为 $\text{H}(x)$ ，规定 $0\cdot\ln0=0$ ；对连续型随机变量 $x$ ，若 $x\sim f(x)$ ，
且 $-\int p(x)\ln p(x)dx$ 有意义，则称其为 $x$ 的熵，也记为 $\text{H}(x)$ 。

# 估计及检验

## 基本概念

定义1：在参数 $\theta$ 的变化范围Q上，定义一个二元非负实值函数 $\text{L}(\theta,a)$ ，
称为损失函数，表示用 $a$ 去估计 $\theta$ 的损失。 $E(L(\theta,a))$ 表示期望平均损失，
称为 $a$ 相应的风险函数，用 $\text{R}(\theta)$ 表示。

定义2：若 $\hat{\theta}_*(x)$ 在估计时使得风险函数 $\text{R}(\theta)$ 最小，则称 
$\hat{\theta}_*(x)$ 是一致最小风险估计。

定义3：若 $\hat{\theta}_*(x)$ 使 
$\rho(\hat{\theta}(x),\pi(\theta))=\min_{\hat{\theta}(x)}\rho(\hat{\theta}(x),\pi(\theta))$ ，则称 
$\hat{\theta}_*(x)$ 是针对 $\pi(\theta)$ 的贝叶斯解。其中 
$\rho(\hat{\theta}(x),\pi(\theta))=\int_{\theta(x)}R(\theta)\pi(\theta)d\theta$ 。

定理1：对给定的损失函数 $\text{L}(\theta,a)$ 和先验分布 $\pi(\theta)$ ，若存在 $p(x|\theta)$ ，
记 $R(\hat{\theta}(x)|x)=\int L(\theta,\hat{\theta}(x))p(x|\theta)\pi(\theta)d\theta$ ，
称为 $\hat{\theta}(x)$ 对 $x$ 的后验风险。当 $R(\hat{\theta}(x)|x)=\min_{\hat{\theta}(x)}R(\hat{\theta}(x)|x)$ ， 
$\hat{\theta}_*(x)$ 就是 $\pi(\theta)$  相应的贝叶斯解。相当于，如果有一个 $\theta$ 的估计
使对每个样本值 $x$ ，后验风险都达到最小，它就是所要求的贝叶斯解。

## 估计

### 最大后验估计

使后验密度 $p(\theta|x)$ 达到最大的估计 $\hat{\theta}(x)$ 为最大后验估计。

### 条件期望估计

用后验分布的条件期望值（即后验分布密度的期望值）去估计参数。当不指明损失函数时，
贝叶斯估计为条件期望估计。

### 区间估计

定义1：设参数 $\theta$ 的后验分布为 $h(\theta|x)$ ，对给定的样本 $x$ 和概率 $1-\alpha$ ，
若存在 $\hat{\theta}_L=\hat{\theta}_L(x)、\hat{\theta}_U=\hat{\theta}_U(x)$ ，使 
$p(\hat{\theta}_L\leq\theta\leq\hat{\theta}_U|x)\geq1-\alpha$ ，则称区间 $[\hat{\theta}_L,\hat{\theta}_U]$ 
为参数 $\theta$ 的可信水平为 $1-\alpha$ 的贝叶斯可信区间，其解释为“ $\theta$ 落入 $[\hat{\theta}_L,\hat{\theta}_U]$ 
的概率为 $1-\alpha$ ”，这与经典统计中有所区别，在经典统计中，$\theta$ 不是随机变量，
而是一个固定的数，置信区间解释为这个区间能覆盖住 $\theta$ 的概率为 $1-\alpha$ 。满足 
$p(\theta\geq\hat{\theta_L})\geq1-\alpha$ 的 $\hat{\theta}_L$ 称为 $1-\alpha$ 的可信下限；
满足 $p(\theta\leq\hat{\theta}_U|x)\geq1-\alpha$ 的 $\hat{\theta}_U$ 称为 $1-\alpha$ 的可信上限。

定义2：设参数 $\theta$ 的后验密度为 $h(\theta|x)$ ，对给定的概率 $1-\alpha$ ，若在直线上
存在这样一个子集 $C$ ，满足： $p(C|x)=1-\alpha、\forall\theta_1\in C,\theta_2\notin C$ ，
总有 $h(\theta_1|x)\geq h(\theta_2|x)$ ，则称 $C$ 为 $\theta$ 的可信水平为 $1-\alpha$ 的
最大后验密度可信集，简称 $1-\alpha$ HPD可信集，若 $c$ 是一个区间，则 $C$ 称为 $1-\alpha$ 
最大后验密度可信区间。

## 假设检验

### 假设检验基本思想

算得贝叶斯后验分布 $h(\theta|x)$ 后，即可计算两个假设的后验概率 $\alpha_i=p(\Theta_i|x)d\theta,\quad i=0,1$ 
，之后比较 $\alpha_0、\alpha_1$ 的大小，当 $\frac{\alpha_0}{\alpha_1}>1$ 时，接受 $\text{H}_0$ ；
当 $\frac{\alpha_0}{\alpha_1}<1$ 时，拒绝 $\text{H}_0$ ；当 $\frac{\alpha_0}{\alpha_1}\approx 1$ 
时，尚需进一步抽样获得先验信息。其中 $\frac{\alpha_0}{\alpha_1}$ 称为后验机会比。举例说明其含义， 
$\frac{\alpha_0}{\alpha_1}=8.14$ 表明 $\Theta_0$ 为真的可能要比 $\Theta_1$ 为真的可能大8.14倍。

### 贝叶斯因子

定义1：设两个假设 $\Theta_0、\Theta_1$ 的先验概率分别为 $\pi_0、\pi_1$ ，后验概率分别为 
$\alpha_0、\alpha_1$ ，则称 $B^{\pi}(x)=\frac{\alpha_0/\alpha_1}{\pi_0/\pi_1}$ 为贝叶斯因子，
表征 $x$ 支持 $\Theta_0$ 的程度。

### 简单假设具体情况

1. 简单假设对简单假设（$\text{H}_0:\theta=\theta_0,\quad \text{H}_1:\theta=\theta_1$）

$\alpha_0=\frac{\pi_0p(x|\theta_0)}{\pi_0p(x|\theta_0)+\pi_1p(x|\theta_1)}, \quad \alpha_1=\frac{\pi_1p(x|\theta_1)}{\pi_0p(x|\theta_0)+\pi_1p(x|\theta_1)}$ ，
则 $\frac{\alpha_0}{\alpha_1}=\frac{\pi_0p(x|\theta_0)}{\pi_1p(x|\theta_1)}$ 。

2. 复杂假设对复杂假设（$\text{H}_0:\theta>1,\quad \text{H}_1:\theta\leq 1$）

$g_0(\theta)\varpropto \pi(\theta)I_{\theta_0}(\theta),\quad g_1(\theta)\varpropto \pi(\theta)I_{\theta_1}(\theta)$ 
，则先验分布 $\pi(\theta)=\pi_0g_0(\theta)+\pi_1g_1(\theta)$ ，其中 $\pi_0、\pi_1$ 分别是 
$\Theta_0、\Theta_1$ 上的先验概率， $g_0、g_1$ 分别是 $\Theta_0、\Theta_1$ 上的概率密度函数，
则 $\frac{\alpha_0}{\alpha_1}=\frac{\int_{\theta_0}p(x|\theta)\pi_0g_0(\theta)d\theta}{\int_{\theta_1}p(x|\theta)\pi_1g_1(\theta)d\theta}$

3. 简单假设对复杂假设（$\text{H}_0:\theta=\theta_0,\quad \text{H}_1:\theta \neq \theta_0$）

当 $\theta$ 连续时，用上述假设是不合适的，应为： 
$\text{H}_0:\theta\in[\theta_0-\varepsilon,\theta_0+\varepsilon];\text{H}_1:\theta\notin [\theta_0-\varepsilon,\theta_0+\varepsilon]$ ，
其中 $\varepsilon$ 为一个很小的数。对 $\theta=\theta_0$ 给一个正概率 $\pi_0$ ，
对 $\theta\notin \theta_0$ 给一个加权密度 $\pi_1g_1(\theta)$ ，即 $\theta$ 的先验密度为 
$\pi(\theta)=\pi_0\text{I}_{\theta_0}(\theta)+\pi_1g_1(\theta)$ ，其中 $\text{I}_{\theta_0}(\theta)$ 为 
$\theta=\theta_0$ 的示性函数， $\pi_1=1-\pi_0$ ， $g_1(\theta)$ 为 $\theta\notin \theta_0$ 
上的一个正概率密度函数。样本 $x$ 的边缘分布 
$m(x)=\int_{\theta}p(x|\theta)\pi(\theta)d\theta=\pi_0p(x|\theta_0)+\pi_1m_1(x)$ ，其中 
$m_1(x)=\int_{\theta\notin\theta_0}p(x|\theta)\pi(\theta)d\theta$ ，从而简单假设与
复杂假设的后验概率为： $\pi(\Theta_0|x)=\frac{\pi_0p(x|\theta_0)}{m(x)}、\pi(\Theta_1|x)=\frac{\pi_1m_1(x)}{m(x)}$ 
，则 $\frac{\alpha_0}{\alpha_1}=\frac{\pi_0p(x|\theta_0)}{\pi_1m_1(x)}$ 。

# 贝叶斯决策

## 决策问题三要素

1. 状态集 $\Theta=\{\theta\}$ ，每个 $\theta$ 表示自然界（社会）会出现的一种状态，
状态集用实数表示为状态参数，由一些实数组成为状态空间；
2. 行动集 $\{a\}$ ，每个 $a$ 表示个人对自然界（社会）采取的行动；
3. 收益函数 $\text{Q}(\theta,a)$ 。

## 决策准则

### 行动的容许性

定义1：在给定的决策问题中，假如存在这样的 $a_2$ 使 
$\forall\theta\in\Theta,\exists \text{Q}(\theta,a_2)\geq\text{Q}(\theta,a_1)$ ，
且至少有一个 $\theta$ 使上述不等式严格成立，则称 $a_1$ 是非容许的；假如不存在
这样的 $a_2$ ，则称 $a_1$ 是容许的；假如 $a_1、a_2$ 的收益函数在 $\Theta$ 上
处处相等，则称 $a_1、a_2$ 是等价的。

### 决策准则

1. 悲观准则：每一行动选最小收益，在所有最小收益中选最大值；
2. 乐观准则：每一行动选最大收益，在所有最大收益中选最小值；
3. 折中准则：选乐观系数 $\alpha\in(0,1)$ ，对每一行动 $a$ ，
计算 $H(a)=\alpha\cdot maxQ(\theta,a)+(1-\alpha)\cdot minQ(\theta,a)$ ，取行动 $a$ 使 $H(a)$ 最大。

### 先验期望准则

每种状态给一个先验概率，每种行动下的所有状态乘先验概率得先验期望收益，选最大的；
期望相同时，比较方差，选方差最小的。

### 损失函数

$L(\theta,a)=maxQ(\theta,a)-Q(\theta,a)$ ，为最大收益减去当前收益；

$L(\theta,a)=w(\theta,a)-minw(\theta,a)$ ，为当前支付减去最小支付。

悲观准则：每个行动选最大损失 $maxL(\theta,a)$ ，所有最大损失中选最小的；

先验期望准则：先验期望损失达到最小的行动为最优行动，若先验期望损失相同，
则选先验方差最小的。

常用损失函数：

1. 平方损失函数： $L(\theta,a)=(a-\theta)^2$ ；
2. 加权平方损失函数： $L(\theta,a)=\lambda(\theta)(a-\theta)^2$
3. 线性损失函数： $L(\theta,a)=\begin{cases} K_0(\theta-a), a\leq\theta\\K_1(\theta-a),a>\theta \end{cases}$
4. 绝对值损失函数： $L(\theta,a)=|\theta-a|$
5. 加权线性函数： 
$L(\theta,a)=\begin{cases} K_0(\theta)(\theta-a),a\leq\theta \\K_1(\theta)(\theta-a),a>\theta \end{cases}$
6. 0-1损失函数： $L(\theta,a)=\begin{cases} 0,|a-\theta|\leq\varepsilon\\ 1,|a-\theta|>\varepsilon \end{cases}$
7. 多元二次损失函数： $L(\theta,a)=(a-\theta)^{\prime}A(a-\theta)$ ，其中 
$\theta^{\prime}=(\theta_1,\cdots,\theta_p),a^\prime=(a_1,\cdots,a_p)，A_{p\times p}>0$
8. 二行动线性决策问题的损失函数：状态 $\theta$ 连续或离散， 
$a=\begin{cases} a_1 & \text{接受}\\ a_2 &\text{拒绝} \end{cases}$ ，在每个行动下的收益都是 
$\theta$ 的线性函数，即 $Q(\theta,a)=\begin{cases}b_1+m_1\theta,a=a_1  \\b_2+m_2\theta,a=a_2 \end{cases}$

### 效用函数

常见的效用曲线：

1. 直线型： $U(\alpha m_1+(1-\alpha)m_2)=\alpha U(m_1)+(1-\alpha) U(m_2)$ （风险中立型）
2. 上凸型： $U(\alpha m_1+(1-\alpha)m_2)>\alpha U(m_1)+(1-\alpha) U(m_2)$ （风险偏好型）
3. 下凸型： $U(\alpha m_1+(1-\alpha)m_2)<\alpha U(m_1)+(1-\alpha) U(m_2)$ （风险厌恶型）
4. 混合型：上面几种类型的混合

# 贝叶斯推断

## 离散随机变量的贝叶斯推断

$x_i$ 的先验概率密度为 $g(x_i)$ ，给定 $x_i$ 的条件下 $y_i$ 的后验概率密度为 $f(y_i|x_i)$ ，
则 $x_i、y_i$ 的联合概率密度为 $f(x_i,y_i)=g(x_i)f(y_i|x_i)$ 。给定 $Y=y_j$ ， $x_i$ 的后验
概率密度为 $g(x_i|y_j)=\frac{g(x_i)f(y_j|x_i)}{\sum_{i=1}^{n_i}g(x_i)f(y_j|x_i)}$

例：容器中有5个球，可能一部分是红色，剩下的是绿色，我们不知道有多少球是红色， $x_i=i$ 表示
红球的个数，则 $i=0,1,2,3,4,5$ 。因为没有任何关于红球的信息，所以假设 $X$ 的先验分布为 
$g(0)=g(1)=g(2)=g(3)=g(4)=g(5)=\frac{1}{6}$ 。我们随机抽取一个球，随机变量 $Y=1$ 表示
抽到红球，否则 $Y=0$ ，则 $P(Y=1|x_i=i)=\frac{i}{5}、P(Y=0|x_i=i)=1-\frac{i}{5}$ 。
根据公式可算得后验概率。

注：贝叶斯学派中，不考虑新数据集，只考虑当前数据集，若出现新数据集，则用当前数据集
算得的后验概率作为新数据集的先验概率再进行新后验概率的计算，等价于直接进行计算 
$f(y_1,y_2|x)=f(y_1|x)f(y_2|x,y_1)$ 。

在贝叶斯理论中，常数乘以先验概率并不会影响贝叶斯后验理论的结果，常数乘以似然也不会
改变贝叶斯理论的结果。因为后验概率与先验概率乘以似然成比例，常数在计算后验概率时被消掉了。

## 二项分布比例的贝叶斯推断

假定 $Y\sim \text{B}(n,\pi)$ ，则给定参数 $\pi$ ， $y$ 的条件分布为 
$f(y|\pi)=C_{n}^{y}\pi^y(1-\pi)^{n-y},\quad y=1,\cdots,n$ 。假定 $y$ 是固定的，
是我们观测的成功的次数， $\pi$ 是可变参数，则 $f(y|\pi)=C_{n}^{y}\pi^y(1-\pi)^{n-y},\quad 0\leq\pi\leq1$ 
。根据以上知识可知， $g(\pi|y)=\frac{g(\pi)f(y|\pi)}{\int_0^1g(\pi)f(y|\pi)d\pi}$ 。

### 使用均匀先验

令 $g(\pi)=1,\quad0\leq\pi\leq1$ ，则 $g(\pi|y)=C_{n}^{y}\pi^y(1-\pi)^{n-y},\quad 0\leq\pi\leq1$ 。

### 使用贝塔先验

假定贝塔先验为 $g(\pi;a,b)=\frac{\Gamma(a+b)}{\Gamma(a)+\Gamma(b)}\pi^{a-1}(1-\pi)^{b-1},\quad 0\leq\pi\leq1$ 
，因为后验和先验乘以似然成比例，因此 $g(\pi|y)\varpropto \pi^{a+y-1}(1-\pi)^{b+n-y-1}$ ，令 
$a^\prime=a+y、b^\prime=b+n-y$ ，易知，后验密度也服从贝塔分布，为 
$g(\pi|y)=\frac{\Gamma(n+a+b)}{\Gamma(y+a)\Gamma(n-y+b)}\pi^{y+a-1}(1-\pi)^{n-y+b-1}$
，易知二项分布的共轭先验是贝塔分布。

二项分布的Jefferys先验是 $\text{Be}(\frac{1}{2},\frac{1}{2})$ 。

### 先验选择准则

1. 当有模糊的先验知识时，选择共轭先验；
2. 通过匹配位置参数（均值）和尺度参数（方差）获得真正的先验知识，选择
共轭先验（样本距等于分布距求参数确定分布）；
3. 构建一般的连续先验（分段函数，不同区间有不同先验）。

### 总结后验分布

位置的测量：

1. 后验众数：极大化后验分布的数，若后验分布连续，则求导令其为0。不足之处为，
可能位于分布的一端或附近，不能代表整体分布；有很多局部极大值极小值求导都为0；
2. 后验中位数： $\int_0^{median}g(\pi|y)d\pi=0.5$ 的 $\text{median}$ ，是很好的位置测量；
3. 后验均值： $m^\prime=\int_0^1\pi g(\pi|y)d\pi$ ，当分布厚尾时，会受到很大影响。

散度测量：

1. 后验方差： $var(\pi|y)=\int_0^1(\pi-m^\prime)^2g(\pi|y)d\pi$
2. 后验标准差： $var(\pi|y)^{\frac{1}{2}}$
3. 后验分位数： $k=\int_{-\infty}^{\pi_k} g(\pi|y)d\pi$ ， $\pi_k$ 是第 $k$ 百分位数
4. 四分位差： $IQR=Q_3-Q_1$

### 估计比例$\pi$

后验均值作为对 $\pi$ 的估计，从后验均方误差 $PMSE(\hat{\pi})=var(\pi|y)+(m^\prime-\pi)^2$ 
中可以看出后验均值作为对 $\pi$ 的估计是最好的。

### 贝叶斯可信区间

贝叶斯可信区间为 $m^\prime+z_{\frac{\alpha}{2}}\times s^\prime$ ，其中 $m^\prime$ 
是后验均值， $s^\prime$ 是后验标准差。

### 贝叶斯学派和频率学派对于比例推断的比较

贝叶斯学派认为参数是固定的未知常数，不是随机变量；而贝叶斯学派认为参数是随机变量，
任何贝叶斯推断都是由后验分布计算的。后验均值作为贝叶斯估计，贝叶斯估计比频率学派
的估计有更小的均方误差，即贝叶斯估计更接近真实值。

贝叶斯统计的假设检验：

1. 单边假设检验（ $\text{H}_0:\pi\leq\pi_0;\text{H}_1:\pi>\pi_0$）

后验概率 $P(\pi\leq\pi_0|y)=\int_0^{\pi_0}g(\pi|x)d\pi$ ，若后验概率小于显著性
水平 $\alpha$ ，则拒绝原假设。

2. 双边假设检验（$\text{H}_0:\pi=\pi_0;\text{H}_1:\pi\neq \pi_0$）

置信区间和双边假设的关系：若不拒绝 $\text{H}_0$ ，则说明 $\pi\in[\pi_0-\varepsilon,\pi_0+\varepsilon]$ ；
若拒绝 $\text{H}_0$ ，则说明 $\pi\notin[\pi_0-\varepsilon,\pi_0+\varepsilon]$ ，其中 $\varepsilon$ 
是个很小的数。贝叶斯统计中，用后验分布进行计算，若 $\pi_0$ 落入可信区间，则接受 $\text{H}_0$ ，否则，
拒绝 $\text{H}_0$ 。

## 正态均值的贝叶斯推断

### 有离散先验均值的贝叶斯理论

将多个观察值一个一个算得的后验和用观察值的均值算得的后验相同。

### 有连续先验的正态均值的贝叶斯理论

1. 正态均值Jeffreys先验： $g(\mu)=1$ 。

单一观测 $y$ ：

$$f(y|\mu)\varpropto \exp(-\frac{(y-\mu)^2}{2\sigma^2}),\quad y|\mu\sim N(\mu,\sigma^2)$$

$$g(\mu|y)\varpropto\exp(-\frac{(\mu-y)^2}{2\sigma^2}),\quad \mu|y\sim N(y,\sigma^2)$$

正态随机样本 $y_1,\cdots,y_n$ ：

$$f(\bar{y}|\mu)\varpropto \exp(-\frac{(\bar{y}-\mu)^2}{2\sigma^2/n}),\quad \bar{y}|\mu\sim N(\mu,\frac{\sigma^2}{n})$$

$$g(\mu|\bar{y})\varpropto \exp(-\frac{(\mu-\bar{y})^2}{2\sigma^2/n}),\quad \mu|\bar{y}\sim N(\bar{y},\frac{\sigma^2}{n})$$

2. 正态均值正态先验： $g(\mu)\varpropto \exp(-\frac{(\mu-m)^2}{2s^2}),\quad \mu\sim N(m,s^2)$

单一观测 $y$ ：

$$f(y|\mu)\varpropto \exp(-\frac{(y-\mu)^2}{2\sigma^2}),\quad \mu\sim N(m,s^2)$$

因此， $g(\mu)f(y|\mu)\varpropto \exp(-\frac{(y-\mu)^2}{2\sigma^2}-\frac{(\mu-m)^2}{2s^2})$

正态分布族简单的更新规则：

后验正态分布的均值和方差分别为： 
$m^\prime=\frac{\sigma^2m+s^2y}{\sigma^2+s^2},\quad (s^\prime)^2=\frac{\sigma^2s^2}{\sigma^2+s^2}$ ，
其中正态先验分布 $\text{N}(m,s^2)$ ， $y\sim N(\mu,\sigma^2)$ 。

进一步将上式化简，可以得到：
$\frac{1}{(s^\prime)^2}=\frac{1}{s^2}+\frac{1}{\sigma^2}$ ;
$m^\prime=\frac{1/s^2}{1/\sigma^2+1/s^2}\times m+\frac{1/\sigma^2}{1/\sigma^2+1/s^2}\times y$ 。
可以发现：后验均值是先验均值和观测值的加权平均，权重是其精度对后验的精度。

下一个观测值的预测密度 $f(y_{n+1}|y_1,\cdots,y_n)\sim N(m^\prime,(s^\prime)^2$ ，其中 
$m^\prime=m_n、(s^\prime)^2=\sigma^2+s_n^2$ ，这里 $m_n=\bar{y}、s_n^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2$ 
，相当于 $\bar{y}\sim N(m_n,s_n^2)$ 。

### 正态均值和贝叶斯可信区间

方差已知时，可信区间为 $m^\prime+z_{\frac{\alpha}{2}}s^\prime$

方差未知时，用 $\hat{\sigma}^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2$ 
估计 $\sigma^2$ 带入上式进行求解。

非正态先验，则后验也不是正态的。 $\int_{\mu_L}^{\mu_U}g(\mu|y_1,\cdots,y_n)d\mu=1-\alpha$ ，
最好的 $\mu_L、\mu_U$ 的估计满足 $g(\mu_L|y_1,\cdots,y_n)=g(\mu_U|y_1,\cdots,y_n)$ 。

### 贝叶斯学派和频率学派对于均值推断的比较

频率学派置信区间和有均值先验的贝叶斯可信区间一样。

正态均值的假设检验：

1. 单侧检验（ $\text{H}_0:\mu<\mu_0;\text{H}_1:\mu\geq\mu_0$ ）

零假设的后验概率 $P(\mu<\mu_0|y_1,\cdots,y_n)=\int_{-\infty}^{\mu}g(\mu|y_1,\cdots,y_n)d\mu$ ，
当 $g(\mu|y_1,\cdots,y_n)\sim N(m^\prime,(s^\prime)^2)$ 时，上式
$=P(\frac{\mu-m^\prime}{s^\prime}\leq\frac{\mu_0-m^\prime}{s^\prime})=P(z\leq\frac{\mu_0-m^\prime}{s^\prime})$
，后验概率小于显著性水平 $\alpha$ 时，拒绝原假设，否则，不拒绝原假设。

2.双侧检验（ $\text{H}_0:\mu=\mu_0;\text{H}_1:\mu\neq\mu_0$ ）

$\mu_0$ 在可信区间内，接受原假设，否则，拒绝原假设。原假设设定标准为原来就有的，
备择假设为希望去检验的。

## 简单线性回归的贝叶斯推断

### 最小二乘回归

最小二乘的基本思想是极小化残差平方和，如果回归方程是 $y=\alpha_0+\beta x$ ，
则其参数估计值 $\begin{cases} \hat{\alpha_0}=\bar{y}-\beta \bar{x} \\ \hat{\beta}=\frac{\bar{xy}-\bar{x}\bar{y}}{\bar{x^2}-{\bar{x}^2}} \end{cases}$ 。
用 $A_0$ 代替 $\alpha_0$ ， $B$ 代替 $\beta$ ，则原式为 $y=A_0+Bx$ ，且 $A_0=\bar{y}-B\bar{x}$ ，
得到 $\bar{y}=A_0+B\bar{x}$ ，将其记为 $A_{\bar{x}}$ ，则最小二乘的等价形式为 
$y=A_{\bar{x}}+B(x-\bar{x})=\bar{y}-B(x-\bar{x})$ ，相应的最小二乘的方差估计为 
$\hat{\sigma}^2=\frac{\sum_{i=1}^n[y_i-(A_\bar{x}+B(x-\bar{x}))]^2}{n-2}$ ，
除以 $n-2$ 是因为有两个待估参数 $A_{\bar{x}}、B$ 。

### 指数增长模型

当遇到经济时间序列时，预测变量是时间 $t$ ，想要看响应变量对 $t$ 的依赖程度，
通常画时间 $t$ 与响应变量之间的散点图，会发现两件事情。首先，响应变量随时间 $t$ 
的增长不再是线性的，第二，点的变异性似乎随着响应变量有相同的增长速率。在 $t$ 
和残差的图中会看的更清楚一点。在这种情况下，指数增长模型会有更好的拟合： 
$u=\exp(\alpha_0+\beta t)$ ，令 $y=\log_e(u)$ ，则方程式变为 $y=\alpha_0+\beta t$ 
是线性形式，可以用最小二乘进行估计，拟合的指数增长模型为 $u=\exp(A_0+Bt)$ ，
其中 $A_0、B$ 是取对数后数据的斜率和截距。

### 简单线性回归假设

1. 均值假设：给定 $x$ 后 $y$ 的条件均值是 $x$ 的未知线性函数，即 $\mu_{y|x}=\alpha_0+\beta x$ ，
其中 $\alpha_0$ 是当 $x=0$ 时的截距；也可以将其表示为： $\mu_{y|x}=\alpha_{\bar{x}}+\beta(x-\bar{x})$ ，
其中 $\alpha_{\bar{x}}$ 是当 $x=\bar{x}$ 时的截距。
2. 误差假设：误差服从零均值同方差的正态分布。
3. 独立性假设：误差项之间相互独立。

### 回归模型的贝叶斯理论

贝叶斯理论可以总结为：后验和先验乘以似然成比例，所以，想要知道后验，必须知道似然和先验。

1. $\beta$ 和 $\alpha_{\bar{x}}$ 的联合似然

观测 $i$ 的似然：

$$likelihood_i(\alpha_{\bar{x},}\beta)\varpropto \exp(-\frac{1}{2\sigma^2}[y_i-(\alpha_{\bar{x}}+\beta(x_i-\bar{x}))]^2)$$

所有样本的似然： 

$$likelihood_{sample}(\alpha_{\bar{x},}\beta)\varpropto \prod_{i=1}^n \exp(-\frac{1}{2\sigma^2}[y_i-(\alpha_{\bar{x}}+\beta(x_i-\bar{x}))]^2)$$

指数部分可以变成求和，因此 ：

$$likelihood_i(\alpha_{\bar{x},}\beta)\varpropto \exp(-\frac{1}{2\sigma^2}[\sum_{i=1}^n[y_i-(\alpha_{\bar{x}}+\beta(x_i-\bar{x}))]^2])$$

分析上式中的指数部分，忽略常数，将其余中的内容加 $\bar{y}$ 再减 $\bar{y}$ ，得到

$$\sum_{i=1}^n(y_i-\bar{y})^2+2\sum_{i=1}^n(y_i-\bar{y})(\bar{y}-(\alpha_{\bar{x}}+\beta(x_i-\bar{x})))+\sum_{i=1}^n(\bar{y}-(\alpha_{\bar{x}}+\beta(x_i-\bar{x})))^2$$

可以将其简化为： 

$$SS_y-2\beta SS_{xy}+\beta^2SS_x+n(\alpha_{\bar{x}}-\bar{y})^2$$

$$SS_y=\sum_{i=1}^n(y_i-\bar{y})^2,SS_{xy}=\sum_{i=1}^n(y_i-\bar{y})(x_i-\bar{x}),SS_x=\sum_{i=1}^n(x_i-\bar{x})^2$$ 

因此，似然函数可以写为：

$$likelihood_{sample}(\alpha_{\bar{x}},\beta)\varpropto \exp(-\frac{1}{2\sigma^2}[SS_y-2\beta SS_{xy}+\beta^2 SS_x+n(\alpha_{\bar{x}}-\bar{y})^2])$$

进一步将上式分解，可以得到

$$liklihood_{sample}(\alpha_{\bar{x}},\beta)\varpropto \exp(-\frac{1}{2\sigma^2/SS_x}[\beta-\frac{SS_{xy}}{SS_x}]^2)\times\exp(-\frac{1}{2\sigma^2/n}[\alpha_{\bar{x}}-\bar{y}]^2)$$

注意到 $\frac{SS_{xy}}{SS_x}=B,\bar{y}=A_{\bar{x}}$ ，因此，可以将联合似然写为个体似然的乘积，即：

$$likelihood_{sample}(\alpha_{\bar{x}},\beta)\varpropto likelihood_{sample}(\alpha{\bar{x}})\times likelihood_{sample}(\beta)$$

因此， $\beta\sim N(B,\frac{\sigma^2}{SS_x}),\alpha_{\bar{x}}\sim N(A_{\bar{x}},\frac{\sigma^2}{n})$ 。

2. $\beta$ 和 $\alpha_{\bar{x}}$ 的联合先验

联合先验可以选用正态先验，也可以选用均匀先验，且 $g(\alpha_{\bar{x}},\beta)=g(\alpha_{\bar{x}})\times g(\beta)$

3. $\beta$ 和 $\alpha_{\bar{x}}$ 的联合后验

联合后验与联合先验乘以似然成比例，

$$g(\alpha_{\bar{x}},\beta|data)\varpropto g(\alpha_{\bar{x}},\beta)\times likelihood_{sample}(\alpha_{\bar{x}},\beta)$$

其中，data 是 $(x_1,y_1),\cdots,(x_n,y_n)$ 。

给定边际后验，上式可写为： 

$$g(\alpha_{\bar{x}},\beta|data)\varpropto g(\alpha_{\bar{x}}|data)\times g(\beta|data)$$。

使用更新公式，就可以找到每个后验的分布，进而求得联合后验分布。

4. 斜率的贝叶斯可信区间

斜率的可信区间为： $m^\prime_{\beta}\pm z_{\frac{\alpha}{2}}\times s^\prime_{\beta}$ 。
通常， $\sigma^2$ 是未知的，因此需要通过 $\hat{\sigma}^2=\frac{\sum_{i=1}^n(y_i-(A_{\bar{x}}+B(x_i-\bar{x})))^2}{n-2}$
进行估计，因此可信区间就变为 $m^\prime_{\beta}\pm t_{\frac{\alpha}{2}}\times s^\prime_{\beta}$ 。
频率学派的置信区间为 $B\pm t_{\frac{\alpha}{2}}\times \frac{\hat{\sigma}}{SS_x}$ 。

5. 斜率的单边假设检验

建立假设： $H_0:\beta\leq\beta_0;H_1:\beta>\beta_0$

检验统计量： $P(\beta\leq\beta_0|data)=\int_{-\infty}^{\beta_0}g(\beta|data)d\beta=P(Z\leq\frac{\beta_0-m^\prime_\beta}{s^\prime_{\beta}})$

判断标准：当计算的P值小于显著性水平 $\alpha$ 时，拒绝原假设，否则，不拒绝原假设。

注：如果使用方差的估计值，则用自由度为 $n-2$ 的 $t$ 分布的值代替 $Z$ 。

6. 斜率的双侧假设检验

建立假设： $H_0:\beta=0;H_1:\beta\neq0$

判断标准：如果0在可信区间内，则拒绝原假设，否则，不拒绝原假设。

### 未来观测值的预测

回归分析的一个重要的目的就是对观测值做出预测，如果给定 $x_{n+1}$ ，则相应的 
$\tilde{y}_{n+1}=\hat{\alpha_{\bar{x}}}+\hat{\beta}(x_{n+1}-\bar{x})$ 。

1. 预测分布

预测分布为： $f(y_{n+1}|x_{n+1},data)=\int\int f(y_{n+1},\alpha_{\bar{x}},\beta|x_{n+1},data)d\alpha_{\bar{x}}d\beta$

首先，需要确定参数和下一个观测值的联合后验分布，为：

$$f(y_{n+1},\alpha_{\bar{x}},\beta|x_{n+1},data)=f(y_{n+1}|\alpha_{\bar{x}},\beta,x_{n+1},data)\times g(\alpha_{\bar{x}},\beta|x_{n+1},data)$$

给定参数 $\alpha_{\bar{x}}、\beta$ 和已知的值 $x_{n+1}$ ，下一个观测值 $y_{n+1}$ 
是回归模型的另一个随机观测，给定参数的值，观测值都是互相独立的，这就意味着，
给定参数，新的观测值并不依赖于数据，参数 $\alpha_{\bar{x}}、\beta$ 
从之前的观测值中计算出来，也不依赖于新的观测值。因此，新观测的联合分布可以简化为：

$$f(y_{n+1},\alpha_{\bar{x}},\beta|x_{n+1},data)=f(y_{n+1}|\alpha_{\bar{x}},\beta,x_{n+1})\times g(\alpha_{\bar{x}},\beta|x_{n+1},data)$$

根据之前的假设，可以得到： $\mu_{n+1}=\alpha_{\bar{x}}+\beta(x_{n+1}-\bar{x})$ ，所以后验分布 
$\mu_{n+1}$ 的均值 $m^\prime_{\mu}=m^\prime_{\alpha_{\bar{x}}}+(x_{n+1}-\bar{x})\times m^\prime_{\beta}$ ，
方差 $(s^\prime_{\mu})^2=(s^\prime_{\alpha_{\bar{x}}})^2+(x_{n+1}-\bar{x})^2\times (s^\prime_{\beta})^2$ 。

经计算，可得 $y_{n+1}\sim N(m^\prime_y,(s^\prime_y)^2)$ ，其中 
$m^\prime_y=m^\prime_\mu,(s^\prime_y)^2=(s^\prime_\mu)^2+\sigma^2$ 。

2. 预测的可信区间

方差已知时，预测值 $y_{n+1}$ 的 $1-\alpha$ 的可信区间为： $m^\prime_y\pm z_{\frac{\alpha}{2}}\times s^\prime_y$

方差未知时，预测值 $y_{n+1}$ 的 $1-\alpha$ 的可信区间为： $m^\prime_y\pm t_{\frac{\alpha}{2}}\times s^\prime_y$

## 具有未知均值和方差的正态贝叶斯推断

### 联合似然函数

$$f(y_1,\cdots,y_n|\mu,\sigma^2)\varpropto \frac{1}{(\sigma^2)^{\frac{1}{2}}}\exp(-\frac{n}{2\sigma^2}(\bar{y-}\mu)^2)\times\frac{1}{(\sigma^2)^{\frac{n-1}{2}}}\exp(-\frac{SS_y}{2\sigma^2})$$

其中， $\mu|\sigma^2\sim N(\bar{y},\frac{\sigma^2}{n})$ ， $\sigma^2$ 是 $ss_y$ 乘以逆卡方分布。

### $\mu、\sigma$ 使用独立Jeffreys先验时的后验

正态均值 $\mu$ 的Jeffreys先验： $g_{\mu}(\mu)=1,\quad -\infty<\mu<\infty$

正态方差 $\sigma^2$ 的Jeffreys先验： $g_{\sigma^2}(\sigma^2)=\frac{1}{\sigma^2},\quad 0<\sigma^2<\infty$

联合先验： $g_{\mu,\sigma^2}(\mu,\sigma^2)=\frac{1}{\sigma^2},\quad \begin{cases}  -\infty<\mu<\infty\\0<\sigma^2<\infty \end{cases}$

联合后验与联合先验乘以似然成比例，则：

$$g_{\mu,\sigma^2}(\mu,\sigma^2|y_1,\cdots,y_n)\varpropto g_{\mu,\sigma^-2}(\mu,\sigma^2)\times f(y_1,\cdots,y_n|\mu,\sigma^2)\varpropto \frac{1}{(\sigma^2)^{\frac{n}{2}+1}}\exp(-\frac{1}{2\sigma^2}[n(\mu-\bar{y})^2+SS_y])$$

若将 $\sigma^2$ 看成唯一参数，则上式为常数 $(n(\mu-\bar{y})^2+SS_y)$ 乘以逆卡方分布。

1. $\mu$ 的边际后验分布

通常我们在做推断时感兴趣的是参数 $\mu$ ，同时将参数 $\sigma^2$ 视为讨厌参数。
去掉讨厌参数的通常做法就是积分积掉它，在这种情况下， $\mu$ 的边际后验分布为：

$$g(\mu|y_1,\cdots,y_n)\varpropto\int_0^{\infty}g(\mu,\sigma^2|y_1,\cdots,y_n)d\sigma^2\varpropto [n(\mu-\bar{y})^2+SS_y]^{-\frac{n}{2}}$$

2. 获得边际后验的另一种方式

定理1：如果 $z、w$ 是独立的随机变量，且分别服从 $N(0,1^2)$ 和 $\chi^2(k)$ ，
则 $u=\frac{z}{\sqrt{\frac{w}{z}}}\sim t(k)$ 。

注意到，我们可以将联合后验写为： 

$$g_{\mu,\sigma^2}(\mu,\sigma^2|y_1,\cdots,y_n)\varpropto \frac{1}{(\sigma^2)^{\frac{1}{2}}}\exp(-\frac{n}{2\sigma^2}(\mu-\bar{y})^2)\times\frac{1}{(\sigma^2)^{\frac{n-1}{2}+1}}\exp(-\frac{SS_y}{2\sigma^2})$$

我们知道： $\mu|\sigma^2\sim N(\bar{y},\frac{\sigma^2}{n})$ , $\sigma^2$ 是 $SS_y$ 
和逆卡方分布的乘积，因此：

$$\frac{\mu-\bar{y}}{\sqrt{\frac{\sigma^2}{n}}}\sim N(0,1^2),\quad \frac{SS_y}{\sigma^2}\sim \chi^2(n-1)$$

因此： $t=\frac{\frac{\mu-\bar{y}}{\sqrt\frac{\sigma^2}{n}}}{\sqrt{\frac{SS_y}{\sigma^2(n-1)}}}=\frac{\mu-m^\prime}{\frac{\hat{\sigma}_B}{\sqrt{n}}}\sim t(k^\prime)$

其中 $\hat{\sigma}_B=\frac{SS_y}{k^\prime}$ 是方差的样本估计值。这就意味着后验均值 
$\mu=m^\prime+\frac{\hat{\sigma}_B}{\sqrt{n^\prime}}\sim ST_{k^\prime}(m^\prime,\frac{\hat{\sigma}_B}{n^\prime})$ 
，其中 $k^\prime=n-1,n^\prime=n,m^\prime=\bar{y}$

### $\mu、\sigma^2$ 使用共轭先验时的后验

1. 联合共轭先验

2. $\mu$ 的边际后验

3. $\mu$ 边际后验的近似

### 有相同未知方差的正态均值的差异

假设 $y1=(y_{11},\cdots,y_{1n_1}),y2=(y_{21},\cdots,y_{2n_2})$ ，
且 $y_1\sim N(\mu_1,\sigma^2)，y_2\sim N(\mu_2,\sigma^2)$ 。

令 $\mu_d=\mu_1-\mu_2$ ，则样本均值差为： $m^\prime_d=\bar{y_1}-\bar{y_2}$ 。
给定 $\sigma^2$ ， $\mu_d$ 的后验分布为 $\mu_d\sim N(m^\prime_d,\sigma^2(\frac{1}{n_1}+\frac{1}{n_2}))$ 。

共轭先验确定精确后验

共轭先验确定近似后验

### 有不同未知方差的正态均值的差异







## 包括马尔可夫链蒙特卡洛的计算的贝叶斯统计

我们可以基于来自后验分布的随机样本进行推断，且随着样本量的增加，后验分布越来越精确。

贝叶斯统计：理论简单，操作困难

贝叶斯理论：后验和先验乘以似然成比例，即 $g(\theta|y)\varpropto g(\theta)f(y|\theta)$ 。
精确后验分布为 $g(\theta|y)=\frac{g(\theta)f(y|\theta)}{\int g(\theta)f(y|\theta)d\theta}$ 。
只有在有限的特殊情况才可以找到后验分布的封闭解，在其他情况下，需要用数值估计。随着 $\theta$ 
维数的增加，这个数值计算过程会很快失去效率，数值积分的精度依赖于高维空间中计算点的位置，
因此，贝叶斯统计在实现的过程中是很困难的。

计算贝叶斯统计学基于使用这些算法从后验分布中获得样本，之后使用后验分布中的随机样本
作为推断的基础。即使在不知道后验分布的时候也可以用。它适用于一般的分布，不局限于
有共轭先验的指数分布族。

### 从后验中抽样的直接方法

1. 逆概率抽样

逆概率抽样依赖于概率积分转换。

定理1：如果 $X$ 是连续随机变量，累积分布函数为 $F_{X}(x)$ ，随机变量 $Y$ 被定义
为 $Y=F_{X}(X)$ ，且其服从 $U(0,1)$ 。

逆概率积分转换： $X=F_X^{-1}(Y)$ 和 $X$ 有相同的分布。实际上，如果我们知道了
一个连续随机变量 $X$  的逆累积分布函数通过从 $U(0,1)$  中抽取随机变量并使用
逆累积概率分布进行转换来得到 $X$ 分布中的随机变量。

例子：假设希望从 $f(x)=\begin{cases} 8x,\quad \text{if}\quad 0\leq x<0.25 \\ \frac{8}{3}-\frac{8}{3}x,\quad \text{if}\quad 0.25\leq x<1 \\ 0,\quad \text{otherwise}\end{cases}$ 中抽取样本，可以算得相应的累计概率分布为

$$F(x)=\begin{cases}0,\quad \text{if} \quad x<0 \\ 4x^2,\quad \text{if}\quad 0\leq x<0.25 \\ \frac{8}{3}x-\frac{4}{3}x^2-\frac{1}{3},\quad \text{if}\quad 0.25\leq x<1 \\ 1,\quad \text{if} \quad x>1   \end{cases}$$

对于 $u\in[0,1]$ ，其反函数为 $F^{-1}(u)=\begin{cases} \frac{\sqrt{u}}{2},\quad \text{if}\quad 0\leq u<0.25 \\ 1-\frac{\sqrt{9-12u}}{4},\quad \text{if} \quad 0.25\le1 x\leq1   \end{cases}$

```{r echo=T,eval=F}
m<-10000  #采样10000个点
u<-runif(m,0,1)  #u~U（0,1）
invcdf.func<-function(u){   #定义逆函数
  if(u>=0 && u<0.25) sqrt(u)/2
  else if(u>=0.25 && u<=1) 1-sqrt(3*(1-u))/2
}
x<-unlist(lapply(u,invcdf.func))
curve(8*x,from=0,to=0.25,xlim=c(0,1),ylim=c(0,1),col="red",xlab="",ylab="")
par(new=TRUE)
curve((8/3)-(8/3)*x,from=0.25,to=1,xlim=c(0,1),ylim=c(0,2),col="red",xlab="",ylab="")
par(new=TRUE)
plot(density(x),xlim=c(0,1),ylim=c(0,2),col="blue",xlab="x",ylab="density")
```

2. 接受-拒绝抽样

思想：我们希望从概率密度函数为 $f(x)$ 的分布中抽取样本是很难的，但是我们
可以非常容易地从概率密度函数为 $g(x)$ 的分布中抽取样本， $g(x)$ 包含 $f(x)$ 。
用数学语言表达为： $f(x)\leq Mg(x)$ ，对所有 $x$ 成立，其中 $M$ 是常数。可以
看出 $Mg(x)$ 必须比 $f(x)$ 大，通常将 $f(x)$ 称为目标密度， $g(x)$ 称为
候选密度或建议密度。有时候可以用分布函数代替密度函数。我们可以使用下面的算法
从 $g(x)$ 中抽样，最终得到 $f(x)$ 的样本：

1. 从 $g(x)$ 中抽取 $x$ ，且 $u\sim U[0,1]$ ；
2. 如果 $u<\frac{f(x)}{Mg(x)}$ ，则接受 $x$ ；
3. 否则，拒绝 $x$ ；
4. 重复1-3，直到抽取到期望的样本容量的样本。

不足：效率比较低，要抽取的样本远远大于想要抽取的样本。如果候选密度接近目标密度，
则比较有效；如果候选密度远离目标密度，则效率较低。

例子：采用1中的例子，令 $M=3$ 。

```{r echo=T,eval=F}
f.x<-function(x){    ## 定义要抽样的分布
  if(x>=0 && x<0.25) 8*x
  else if(x>=0.25 && x<=1) 8/3-8/3*x
  else 0
}
g.x<-function(x){   ## 定义均匀分布，乘以常数M后可以完全把f(x)盖住
  if(x>=0 && x<=1) 1
  else 0
}
M<-3
m<-10000
n.draws<-0
draws<-c()
x.grid<-seq(0,1,by=0.01)
while(n.draws<m){   ##当样本量小于规定的样本量时，进行循环
  x.c<-runif(1,0,1)
  accept.prob<-f.x(x.c)/(M*g.x(x.c))
  u<-runif(1,0,1)
  if(accept.prob>=u){
    draws<-c(draws,x.c)
    n.draws<-n.draws+1
  }
}
## 绘图
curve(8*x,from=0,to=0.25,xlim=c(0,1),ylim=c(0,2),col="red",xlab="",ylab="",main="")
par(new=TRUE)
curve((8/3)-(8/3)*x,from=0.25,to=1,xlim=c(0,1),ylim=c(0,2),col="red",xlab="",ylab="",main="")
par(new=TRUE)
plot(density(draws),xlim=c(0,1),ylim=c(0,2),col="blue",xlab="x,N=10000",ylab="density")
#text.legend<-c("Target Density","Rejection Sampling")
3legend("topright",legend=text.legend,lty=c(1,1),col=c("red","blue"))
```

3. 自适应拒绝抽样

自适应拒绝抽样的本质很容易理解，我们根据来自拒绝抽样的拒绝建议的信息自动更新我们
的候选密度。实施起来更加复杂。要注意的是，自适应采样仅仅适用于对数凹分布函数。通常，
一个函数 $f(x)$ 是凹的，如果 $f((1-t)x+ty)\geq(1-t)f(x)+tf(y),\forall t\in(0,1)$ 。
一个函数是对数凹函数如果 $h(x)=log(f(x))$ 遵循相同的不等式。另一个判断标准就是
如果 $f^{\prime\prime}(x)=0$ ，则 $f(x)$ 就是凹函数。

自适应拒绝采样算法：

1. 找到 $h(x)=h(x;\theta)=\log f(x;\theta)$ ，其中 $\theta$ 是描述分布的参数向量；
2. 计算 $h^\prime(x)$ ，解 $h^\prime(x)=0$ 计算极大值点 $x_{max}$ ；
3. 选择两个任意的点 $x_0、x_1$ ，使得 $x_0<x_{max}$ 且 $x_1>x_{max}$ ；
4. 计算切线 $t_0(x)、t_1(x)$ 。切线 $t_i(x_i)$ 是通过点 $(x_i,h(x_i))$ 且斜率
为 $h^\prime(x_i)$ 的线，即：

$$t_i(x)=h^\prime(x_i)x+(h(x_i)-h^\prime(x_i)x)$$

每一条切线都被描述为包含截距和斜率的列向量；
5. 通过指数化 $t_0(x)、t_1(x)$ 计算包络密度 $g_0(x)$ 

$$g_0(x)=\begin{cases}  \exp(t_0(x))=\exp(\alpha_0+\beta_0x) &  \text{for}\quad -\infty<x<x_{max} \\ \exp(t_1(x))=\exp(\alpha_1+\beta_1x) & \text{for} \quad x_{max}\leq x<+\infty   \end{cases}$$
6. 计算累积包络密度 $G_0(x)$ ， $G_0(x)=\int_{-\infty}^x g_0(t)dt$ ，
为了使 $G_0(x)$ 下的面积为1， $k_0=\frac{1}{G_0(+\infty)}$ ；
7. 计算逆累积概率分布 $G_0^{-1}(p)$ ，即 $G_0^{-1}(k_0G_0(x))=x$ ；
8. 样本 $(u,v)\sim U(0,1)$ ；
9. 令 $x=G_0^{-1}(v)$ ；
10. 如果 $u<\frac{f(x)}{g_0(x)}$ ，接受 $x$ 作为目标分布中的随机变量，如果达到了样本量要求，则停止抽样；否则，重复8-10；
11. 否则，增加 $x$ 为切线点，重复4-11。

例子：

```{r echo=T,eval=F}
f<-function(x,a,b){
  (a-1)*log(x)+(b-1)*log(1-x)
}
fprima<-function(x,a,b){   ##求导
  (a-1)/x-(b-1)/(1-x)
}
library(ars)
mysample<-ars(20000,f,fprima,x=c(0.3,0.6),m=2,lb=TRUE,xlb=0,ub=TRUE,xub=1,a=1.3,b=2.7)
hist(mysample,freq=F)
curve(dbeta(x,1.3,2.7),add=T,col="red")
```


### 抽样-重要性抽样

重要性抽样通常发生在人们希望去估计罕见事件的发生的情况下，重要性抽样通过从重要密度
中抽取样本并相应的加权采样解决了这个问题。

如果 $X$ 是随机变量，概率密度函数为 $p(x)$ ， $f(X)$ 是 $X$ 的某些函数，则 $f(X)$ 的
期望为 $E[f(X)]=\int_{-\infty}^{+\infty}f(x)p(x)dx$ 。如果 $h(x)$ 是一个概率密度，它的
条件就包含了 $p(x)$ 的条件，比如 $h(x)>0,\forall x,p(x)>0$ ，则积分可以被重新写为： 
$E[f(X)]=\int_{-\infty}^{+\infty}f(x)\frac{p(x)}{h(x)}h(x)dx$ ， $h(x)$ 是重要密度， 
$\frac{p(x)}{h(x)}$ 被称为似然比。因此，如果我们从 $h(x)$ 中抽取足够大的样本，积分可以
由下式进行估计： $E[f(X)]=\frac{1}{N}\sum_{i=1}^n w_if(x_i)dx$ ，其中 $w_i=\frac{g(x_i)}{h(x_i)}$ 
是重要权重。这个效率与重要密度接近目标密度的程度有关。选择好的重要密度通常比较困难。

当我们想要从未缩放的后验密度中抽取样本时，重要性采样是比较有用的。然而，这并没有帮助
我们从后验密度中抽取样本。我们需要稍微拓展一下它：

1. 从重要密度中抽取大样本 $\theta=(\theta_1,\cdots,\theta_N)$ ；
2. 计算每个值的重要权重： $w_i=\frac{p(\theta_i)}{h(\theta_i)}$ ;
3. 计算标准化权重： $r_i=\frac{w_i}{\sum_{i=1}^nw_i}$ ;
4. 用给定 $r_i$ 时的概率从 $\theta$ 中抽取样本量为 $N^\prime$ 的样本。

这个重抽样结合了重要权重，因此将其称为抽样重要再抽样或SIR方法。这种方法有时被称为
贝叶斯自主法。如果 $N$ 比较小的话， $N^\prime$ 会更小，否则会包含更多的重复值。
然而，当 $N$ 比较大的时候， $N^\prime$ 上的限制就可以放松。

### 马尔科夫蒙特卡洛方法（MCMC）

这种方法可以从精确后验分布中抽取样本，即使仅仅有后验分布的比例形式。MCMC方法可以
使用在有很多参数的复杂模型中。

1. 马尔可夫链

从一个状态转移到另一个状态，下一个状态只与当前状态有关，与过去的状态都没有关系。
这个状态的集合被称为状态空间。我们仅使用马尔可夫链，状态转移矩阵在每一步中都是
相同的，这种链叫作时间不变。最终马尔可夫链会处于一个不变的状态。

在MCMC中，我们需要找到一个有长期分布的马尔可夫链，其分布与后验分布 $g(\theta|y)$ 
的分布相同。状态的集合为参数空间，这个集合中包含所有可能参数的值。

有两种方法去做这件事情：M-H算法和Gibbs采样。M-H采样是来自每对样本稳态概率平衡的概念。
M-H采样也可以用在下列场合：给定其他块参数的值，所有参数只出现一次或每一个块中的参数
都以块为单位。Gibbs采样循环参数，在给定其他参数和数据的最新值的情况下，依次从该参数
的条件分布中对每个参数进行采样。条件分布可能很难去获得。

2. 单一参数的M-H算法

M-H算法旨在从一些目标密度中抽取样本。如果 $g(\theta|y)$ 是一个未缩放的后验分布， 
$q(\theta,\theta^\prime)$ 是候选密度，则转移概率定义为： 
$\alpha(\theta,\theta^\prime)=\min(1,\frac{g(\theta^\prime|y)q(\theta^\prime,\theta)}{g(\theta|y)q(\theta,\theta^\prime)})$ ，
将会满足对称转移的要求。

M-H算法的步骤：

1. 设定初始值 $\theta^{(0)}$ ；
2. 对 $n=1,\cdots,n$ 
- 从 $q(\theta^{(n-1)},\theta^\prime)$ 中抽取 $\theta^\prime$ ；
- 计算概率 $\alpha(\theta^{(n-1)},\theta^\prime)$ ；
- 从 $U(0,1)$ 中抽取 $u$ ；
- 如果 $u<\alpha(\theta^{(n-1)},\theta^\prime)$ ，令 $\theta^{(n)}=\theta^\prime$ ，
否则，令 $\theta^{(n)}=\theta^{(n-1)}$ 。

例：提议分布为 $Be(1,1)$ ，观测数据从正态混合分布 $0.2N(0,1)+0.8N(5,1)$ 中产生。
```{r echo=T,eval=F}
m<-5000  ##链的长度
a<-1
b<-1
xt<-numeric(m)
n<-30  ##样本大小
mu<-c(0,5)
sigma<-c(1,1)
# 产生观测样本
i<-sample(1:2,size=n,replace=TRUE,prob=c(0.2,0.8))
x<-rnorm(n,mu[i],sigma[i])
# 产生独立的采样器链
u<-runif(m)
y<-rbeta(m,a,b)   ##建议分布
xt[1]<-0.5
for(i in 2:m){
  fy<-y[i]*dnorm(x,mu[1],sigma[1])+(1-y[i])*dnorm(x,mu[2],sigma[2])
  fx<-xt[i-1]*dnorm(x,mu[1],sigma[2])+(1-xt[i-1])*dnorm(x,mu[2],sigma[2])
  alpha<-prod(fy/fx)*(xt[i-1]^(a-1)*(1-xt[i-1])^(b-1))/(y[i]^(a-1)*(1-y[i])^(b-1))
  if(u[i]<=alpha) xt[i]=y[i]
  else xt[i]=xt[i-1]
}
##链的路径图和丢掉100个样本后的直方图
plot(xt,type="l",ylab="p")
hist(xt[101:m],main="",xlab="p",prob=TRUE)
print(mean(xt[101:m]))
```


3. Gibbs采样

Gibbs采样和多参数问题更加相关。M-H采样随着参数的增加算法的接受率会下降。Gibbs采样尤其
适用于分层模型，可以很好的定义模型中参数之间的相依性。

假定我们使用真实的条件密度作为候选密度 $q(\theta_j,\theta_j^\prime|\theta_{-j})=g(\theta_j|\theta_{-j},y)$ ，
其中 $\theta_{-j}$ 表示包括第 $j$ 个参数的所有参数的集合。因此，
在第 $n$ 步 $\theta_j$ 的接受概率为

$$\alpha(\theta_j^{(n-1)},\theta_j^\prime|\theta_{-j}^{(n)})=\min (1,\frac{g(\theta_j^\prime|\theta_{-j},y)q(\theta_j^\prime,\theta_j|\theta_{-j})}{g(\theta_j|\theta_{-j},y)q(\theta_j,\theta_{-j}|\theta_{-j})})=1$$

例：

使用Gibbs抽样产生二元正态分布 $N(\mu_1,\mu_2,\sigma^2_1,\sigma^2_2,\rho)$ 随机数

```{r echo=T,eval=F}
# 初始化常数和参数
N<-5000   #链的长度
burn<-1000   #收敛到目标分布的长度
X<-matrix(0,N,2)   #N行2列元素为0的矩阵，双变量样本
rho<--0.75
mu1<-0
mu2<-2
sigma1<-1
sigma2<-0.5
s1<-sqrt(1-rho^2)*sigma1
s2<-sqrt(1-rho^2)*sigma2
# 产生链
X[1,]<-c(mu1,mu2)
for(i in 2:N){
  x2<-x[i-1,2]   ##i-1?
  m1<-mu1+rho*(x2-mu2)*sigma1/sigma2
  X[i,1]<-rnorm(1,m1,s1)
  x1<-X[i,1]     ##i?
  m2<-mu2+rho*(x1-mu1)*sigma2/sigma1
  X[i,2]<-rnorm(1,m2,s2)
}
b<-burn+1
x<-X[b:N,]
#比较样本统计量和参数
colMeans(x)
cov(x)
cor(x)
plot(x,main="",cex=.5,xlab=bquote(X[1]),ylab=bquote(X[2]),ylim=range(x[,2]))
```

### 切片抽样

对于具有单峰的一维密度特别有效。每一步都有两个阶段，在第 $i$ 步，首先给定
当前参数 $\theta_{i-1}$ ，从均匀分布 $U(0,c)$ 中得到辅助变量 $g_i$ ，
其中 $c=g(\theta_{i-1})$ ，这是从当前值 $\theta_{i-1}$ 中均匀的抽取垂直切片；
之后，给定当前辅助变量值从均匀分布 $U(a,b)$ 中抽取 $\theta_i$ ，其中 $g(a)=g(b)=g_i$ 。
链的长远分布收敛于参数密度下的随机点，因此，水平分量的抽取来自参数密度。



# 参考文献
[//]: # (\bibliography{Bibfile})